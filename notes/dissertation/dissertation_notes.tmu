<TMU|<tuple|1.1.0|2025.0.3>>

<style|<tuple|generic|reduced-margins>>

<\body>
  <doc-data|<doc-title|Dissertation Notes>|<doc-author|<author-data|<author-name|Jack Yansong Li>|<author-email|yli340@uic.edu>|<\author-affiliation>
    University of Illinois Chicago
  </author-affiliation>>>>

  <section|Mathematical formulation for helpers in learning systems>

  <\question>
    Can we integrate gradient descent algorithms into the setting with backup helpers?
  </question>

  We consider the following composite optimization problem with objectives being

  <\equation*>
    f=<wide|f|^>+r.
  </equation*>

  The term <math|r> can be considered as a backup helper. For example, for <math|r\<equiv\>0>, the helper do nothing with the agent's reward function. For input <math|x> such that <math|r<around*|(|x|)>\<neq\>0>, the helper gives a hint to the agent by drifting his reward function by <math|r<around*|(|x|)>>. However, the function <math|r> is unknown and cannot be learned by the agent.

  Note that this is different from imitation learning since the agent does not directly mimic the helpers behavior.\ 

  \;

  A trivial gradient update rule is:

  <\equation*>
    f<around*|(|x<rsup|k+1>|)>\<leftarrow\>f<around*|(|x<rsup|k>|)>-\<eta\>\<nabla\><wide|f|^><around*|(|x|)>
  </equation*>

  \;
</body>

<\initial>
  <\collection>
    <associate|page-medium|paper>
    <associate|page-screen-margin|false>
  </collection>
</initial>

<\references>
  <\collection>
    <associate|auto-1|<tuple|1|1>>
  </collection>
</references>

<\auxiliary>
  <\collection>
    <\associate|toc>
      <vspace*|1fn><with|font-series|<quote|bold>|math-font-series|<quote|bold>|1<space|2spc>Mathematical formulation for helpers in learning systems> <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>> <no-break><pageref|auto-1><vspace|0.5fn>
    </associate>
  </collection>
</auxiliary>
