@inproceedings{ahmed_regret_2013,
  title = {Regret Based {{Robust Solutions}} for {{Uncertain Markov Decision Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ahmed, Asrar and Varakantham, Pradeep and Adulyasak, Yossiri and Jaillet, Patrick},
  year = {2013},
  volume = {26},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-03-08},
  abstract = {In this paper, we seek robust policies for uncertain Markov Decision Processes (MDPs). Most robust optimization approaches for these problems have focussed on the computation of \{{\textbackslash}em maximin\} policies which maximize the value corresponding to the worst realization of the uncertainty. Recent work has proposed \{{\textbackslash}em minimax\} regret as a suitable alternative to the \{{\textbackslash}em maximin\} objective for robust optimization.  However, existing algorithms for handling \{{\textbackslash}em minimax\} regret are restricted to models with uncertainty over rewards only.  We provide algorithms that employ sampling to improve across multiple dimensions: (a) Handle uncertainties over both transition and reward models; (b) Dependence of model uncertainties across state, action pairs and decision epochs; (c) Scalability and quality bounds. Finally, to demonstrate the empirical effectiveness of our sampling approaches, we provide comparisons against benchmark algorithms on two domains from literature. We also provide a Sample Average Approximation (SAA) analysis to compute a posteriori error bounds.},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/4QVP5XJT/Ahmed et al. - 2013 - Regret based Robust Solutions for Uncertain Markov.pdf}
}

@article{albrecht_belief_2016,
  title = {Belief and {{Truth}} in {{Hypothesised Behaviours}}},
  author = {Albrecht, Stefano V. and Crandall, Jacob W. and Ramamoorthy, Subramanian},
  year = {2016},
  month = jun,
  journal = {Artificial Intelligence},
  volume = {235},
  eprint = {1507.07688},
  primaryclass = {cs},
  pages = {63--94},
  issn = {00043702},
  doi = {10.1016/j.artint.2016.02.004},
  urldate = {2023-04-30},
  abstract = {There is a long history in game theory on the topic of Bayesian or "rational" learning, in which each player maintains beliefs over a set of alternative behaviours, or types, for the other players. This idea has gained increasing interest in the artificial intelligence (AI) community, where it is used as a method to control a single agent in a system composed of multiple agents with unknown behaviours. The idea is to hypothesise a set of types, each specifying a possible behaviour for the other agents, and to plan our own actions with respect to those types which we believe are most likely, given the observed actions of the agents. The game theory literature studies this idea primarily in the context of equilibrium attainment. In contrast, many AI applications have a focus on task completion and payoff maximisation. With this perspective in mind, we identify and address a spectrum of questions pertaining to belief and truth in hypothesised types. We formulate three basic ways to incorporate evidence into posterior beliefs and show when the resulting beliefs are correct, and when they may fail to be correct. Moreover, we demonstrate that prior beliefs can have a significant impact on our ability to maximise payoffs in the long-term, and that they can be computed automatically with consistent performance effects. Furthermore, we analyse the conditions under which we are able complete our task optimally, despite inaccuracies in the hypothesised types. Finally, we show how the correctness of hypothesised types can be ascertained during the interaction via an automated statistical analysis.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,I.2.11},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/Y7VXWSQA/Albrecht et al. - 2016 - Belief and Truth in Hypothesised Behaviours.pdf;/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/KEN4KWIM/1507.html}
}

@misc{albrechtAreYouDoing2019,
  title = {Are {{You Doing What I Think You Are Doing}}? {{Criticising Uncertain Agent Models}}},
  shorttitle = {Are {{You Doing What I Think You Are Doing}}?},
  author = {Albrecht, Stefano V. and Ramamoorthy, S.},
  year = {2019},
  month = jul,
  number = {arXiv:1907.01912},
  eprint = {1907.01912},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.01912},
  urldate = {2024-02-19},
  abstract = {The key for effective interaction in many multiagent applications is to reason explicitly about the behaviour of other agents, in the form of a hypothesised behaviour. While there exist several methods for the construction of a behavioural hypothesis, there is currently no universal theory which would allow an agent to contemplate the correctness of a hypothesis. In this work, we present a novel algorithm which decides this question in the form of a frequentist hypothesis test. The algorithm allows for multiple metrics in the construction of the test statistic and learns its distribution during the interaction process, with asymptotic correctness guarantees. We present results from a comprehensive set of experiments, demonstrating that the algorithm achieves high accuracy and scalability at low computational costs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/WHSTW6ZY/Albrecht and Ramamoorthy - 2019 - Are You Doing What I Think You Are Doing Criticis.pdf;/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/U36HP9MH/1907.html}
}

@misc{albrechtEmpiricalStudyPractical2019,
  title = {An {{Empirical Study}} on the {{Practical Impact}} of {{Prior Beliefs}} over {{Policy Types}}},
  author = {Albrecht, Stefano V. and Crandall, Jacob W. and Ramamoorthy, Subramanian},
  year = {2019},
  month = jul,
  number = {arXiv:1907.05247},
  eprint = {1907.05247},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.05247},
  urldate = {2023-04-30},
  abstract = {Many multiagent applications require an agent to learn quickly how to interact with previously unknown other agents. To address this problem, researchers have studied learning algorithms which compute posterior beliefs over a hypothesised set of policies, based on the observed actions of the other agents. The posterior belief is complemented by the prior belief, which specifies the subjective likelihood of policies before any actions are observed. In this paper, we present the first comprehensive empirical study on the practical impact of prior beliefs over policies in repeated interactions. We show that prior beliefs can have a significant impact on the long-term performance of such methods, and that the magnitude of the impact depends on the depth of the planning horizon. Moreover, our results demonstrate that automatic methods can be used to compute prior beliefs with consistent performance effects. This indicates that prior beliefs could be eliminated as a manual parameter and instead be computed automatically.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/SC7IAB7K/Albrecht et al. - 2019 - An Empirical Study on the Practical Impact of Prio.pdf;/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/NVMAAEAC/1907.html}
}

@inproceedings{alt_correlation_2019,
  title = {Correlation {{Priors}} for {{Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Alt, Bastian and {\v S}o{\v s}i{\'c}, Adrian and Koeppl, Heinz},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-02-22},
  abstract = {Many decision-making problems naturally exhibit pronounced structures inherited from the characteristics of the underlying environment. In a Markov decision process model, for example, two distinct states can have inherently related semantics or encode resembling physical state configurations. This often implies locally correlated transition dynamics among the states. In order to complete a certain task in such environments, the operating agent usually needs to execute a series of temporally and spatially correlated actions. Though there exists a variety of approaches to capture these correlations in continuous state-action domains, a principled solution for discrete environments is missing. In this work, we present a Bayesian learning framework based on P{\'o}lya-Gamma augmentation that enables an analogous reasoning in such cases. We demonstrate the framework on a number of common decision-making related problems, such as imitation learning, subgoal extraction, system identification and Bayesian reinforcement learning. By explicitly modeling the underlying correlation structures of these problems, the proposed approach yields superior predictive performance compared to correlation-agnostic models, even when trained on data sets that are an order of magnitude smaller in size.},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/4VTG9JTN/Alt et al. - 2019 - Correlation Priors for Reinforcement Learning.pdf}
}

@incollection{baumeister_survey_2022,
  title = {A {{Survey}} of {{Ad Hoc Teamwork Research}}},
  booktitle = {Multi-{{Agent Systems}}},
  author = {Mirsky, Reuth and Carlucho, Ignacio and Rahman, Arrasy and Fosong, Elliot and Macke, William and Sridharan, Mohan and Stone, Peter and Albrecht, Stefano V.},
  editor = {Baumeister, Dorothea and Rothe, J{\"o}rg},
  year = {2022},
  volume = {13442},
  pages = {275--293},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-20614-6_16},
  urldate = {2023-04-27},
  abstract = {Ad hoc teamwork is the research problem of designing agents that can collaborate with new teammates without prior coordination. This survey makes a two-fold contribution: First, it provides a structured description of the different facets of the ad hoc teamwork problem. Second, it discusses the progress that has been made in the field so far, and identifies the immediate and long-term open problems that need to be addressed in ad hoc teamwork.},
  isbn = {978-3-031-20613-9 978-3-031-20614-6},
  langid = {english},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/9YM8QPDM/Mirsky et al. - 2022 - A Survey of Ad Hoc Teamwork Research.pdf}
}

@book{bertsekas_convex_2009,
  title = {Convex Optimization Theory},
  editor = {Bertsekas, Dimitri P.},
  year = {2009},
  publisher = {Athena Scientific},
  address = {Belmont, Mass},
  isbn = {978-1-886529-31-1},
  langid = {english},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/TY6BTRB2/Bertsekas - 2009 - Convex optimization theory.pdf}
}

@inproceedings{buening_minimax-bayes_2023,
  title = {Minimax-{{Bayes Reinforcement Learning}}},
  booktitle = {Proceedings of {{The}} 26th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Buening, Thomas Kleine and Dimitrakakis, Christos and Eriksson, Hannes and Grover, Divya and Jorge, Emilio},
  year = {2023},
  month = apr,
  pages = {7511--7527},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-02-22},
  abstract = {While the Bayesian decision-theoretic framework offers an elegant solution to the problem of decision making under uncertainty, one question is how to appropriately select the prior distribution. One idea is to employ a worst-case prior. However, this is not as easy to specify in sequential decision making as in simple statistical estimation problems. This paper studies (sometimes approximate) minimax-Bayes solutions for various reinforcement learning problems to gain insights into the properties of the corresponding priors and policies. We find that while the worst-case prior depends on the setting, the corresponding minimax policies are more robust than those that assume a standard (i.e. uniform) prior.},
  langid = {english},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/N7HJD9GV/Buening et al. - 2023 - Minimax-Bayes Reinforcement Learning.pdf}
}

@article{canaan_generating_2023,
  title = {Generating and {{Adapting}} to {{Diverse Ad-Hoc Cooperation Agents}} in {{Hanabi}}},
  author = {Canaan, Rodrigo and Gao, Xianbo and Togelius, Julian and Nealen, Andy and Menzel, Stefan},
  year = {2023},
  month = jun,
  journal = {IEEE Trans. Games},
  volume = {15},
  number = {2},
  eprint = {2004.13710},
  primaryclass = {cs},
  pages = {228--241},
  issn = {2475-1502, 2475-1510},
  doi = {10.1109/TG.2022.3169168},
  urldate = {2023-11-13},
  abstract = {Hanabi is a cooperative game that brings the problem of modeling other players to the forefront. In this game, coordinated groups of players can leverage pre-established conventions to great effect, but playing in an ad-hoc setting requires agents to adapt to its partner's strategies with no previous coordination. Evaluating an agent in this setting requires a diverse population of potential partners, but so far, the behavioral diversity of agents has not been considered in a systematic way. This paper proposes Quality Diversity algorithms as a promising class of algorithms to generate diverse populations for this purpose, and generates a population of diverse Hanabi agents using MAP-Elites. We also postulate that agents can benefit from a diverse population during training and implement a simple "meta-strategy" for adapting to an agent's perceived behavioral niche. We show this meta-strategy can work better than generalist strategies even outside the population it was trained with if its partner's behavioral niche can be correctly inferred, but in practice a partner's behavior depends and interferes with the meta-agent's own behavior, suggesting an avenue for future research in characterizing another agent's behavior during gameplay.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/9RUUYWWN/Canaan et al. - 2023 - Generating and Adapting to Diverse Ad-Hoc Cooperat.pdf;/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/4JR5HQKT/2004.html}
}

@inproceedings{castellini_explaining_2020,
  title = {Explaining the {{Influence}} of {{Prior Knowledge}} on {{POMCP Policies}}},
  booktitle = {Multi-{{Agent Systems}} and {{Agreement Technologies}}},
  author = {Castellini, Alberto and Marchesini, Enrico and Mazzi, Giulio and Farinelli, Alessandro},
  editor = {Bassiliades, Nick and Chalkiadakis, Georgios and {de Jonge}, Dave},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {261--276},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-66412-1_17},
  abstract = {Partially Observable Monte Carlo Planning is a recently proposed online planning algorithm which makes use of Monte Carlo Tree Search to solve Partially Observable Monte Carlo Decision Processes. This solver is very successful because of its capability to scale to large uncertain environments, a very important property for current real-world planning problems. In this work we propose three main contributions related to POMCP usage and interpretability. First, we introduce a new planning problem related to mobile robot collision avoidance in paths with uncertain segment difficulties, and we show how POMCP performance in this context can take advantage of prior knowledge about segment difficulty relationships. This problem has direct real-world applications, such as, safety management in industrial environments where human-robot interaction is a crucial issue. Then, we present an experimental analysis about the relationships between prior knowledge provided to the algorithm and performance improvement, showing that in our case study prior knowledge affects two main properties, namely, the distance between the belief and the real state, and the mutual information between segment difficulty and action taken in the segment. This analysis aims to improve POMCP explainability, following the line of recently proposed eXplainable AI and, in particular, eXplainable planning. Finally, we analyze results on a synthetic case study and show how the proposed measures can improve the understanding about internal planning mechanisms.},
  isbn = {978-3-030-66412-1},
  langid = {english},
  keywords = {Explainable artificial intelligence,eXplainable planning,Planning under uncertainty,POMCP,POMDP,XAI},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/AP2JI4JP/Castellini et al. - 2020 - Explaining the Influence of Prior Knowledge on POM.pdf}
}

@article{castro_equivalence_nodate,
  title = {Equivalence {{Relations}} in {{Fully}} and {{Partially Observable Markov Decision Processes}}},
  author = {Castro, Pablo Samuel and Panangaden, Prakash and Precup, Doina},
  abstract = {We explore equivalence relations between states in Markov Decision Processes and Partially Observable Markov Decision Processes. We focus on two different equivalence notions: bisimulation [Givan et al., 2003] and a notion of trace equivalence, under which states are considered equivalent if they generate the same conditional probability distributions over observation sequences (where the conditioning is on action sequences). We show that the relationship between these two equivalence notions changes depending on the amount and nature of the partial observability. We also present an alternate characterization of bisimulation based on trajectory equivalence.},
  langid = {english},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/IB9FRZ5Z/Castro et al. - Equivalence Relations in Fully and Partially Obser.pdf}
}

@article{doshi_modeling_2012,
  title = {Modeling {{Human Recursive Reasoning Using Empirically Informed Interactive Partially Observable Markov Decision Processes}}},
  author = {Doshi, Prashant and Qu, Xia and Goodie, Adam S. and Young, Diana L.},
  year = {2012},
  month = nov,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
  volume = {42},
  number = {6},
  pages = {1529--1542},
  issn = {1558-2426},
  doi = {10.1109/TSMCA.2012.2199484},
  urldate = {2024-02-19},
  abstract = {Recursive reasoning of the form what do I think that you think that I think (and so on) arises often while acting in multiagent settings. Previously, multiple experiments studied the level of recursive reasoning generally displayed by humans while playing sequential general-sum and fixed-sum, two-player games. The results show that subjects experiencing a general-sum strategic game display first or second level of recursive thinking with the first level being more prominent. However, if the game is made simpler and more competitive with fixed-sum payoffs, subjects predominantly attributed first-level recursive thinking to opponents thereby acting using second level. In this article, we model the behavioral data obtained from the studies using the interactive partially observable Markov decision process, appropriately simplified and augmented with well-known models simulating human learning and decision. We experiment with data collected at different points in the study to learn the models parameters. Accuracy of the predictions by our models is evaluated by comparing them with the observed study data, and the significance of the fit is demonstrated by comparing the mean squared error of our model predictions with those of a random hypothesis. Accuracy of the predictions by the models suggest that these could be viable ways for computationally modeling strategic behavioral data in a general way. While we do not claim the cognitive plausibility of the models in the absence of more evidence, they represent promising steps toward understanding and computationally simulating strategic human behavior.},
  keywords = {Computational modeling,Computational models,Data models,Decision making,Games,human decision making,Markov processes,Predictive models,recursive reasoning,strategic games},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/ZPPV83UX/Doshi et al. - 2012 - Modeling Human Recursive Reasoning Using Empirical.pdf}
}

@article{ferns_bisimulation_2011,
  title = {Bisimulation {{Metrics}} for {{Continuous Markov Decision Processes}}},
  author = {Ferns, Norm and Panangaden, Prakash and Precup, Doina},
  year = {2011},
  month = jan,
  journal = {SIAM J. Comput.},
  volume = {40},
  number = {6},
  pages = {1662--1714},
  issn = {0097-5397, 1095-7111},
  doi = {10.1137/10080484X},
  urldate = {2024-02-22},
  langid = {english},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/XHNBKL4M/Ferns et al. - 2011 - Bisimulation Metrics for Continuous Markov Decisio.pdf}
}

@article{fruit_exploration-exploitation_nodate,
  title = {Exploration-Exploitation Dilemma in {{Reinforcement Learning}} under Various Form of Prior Knowledge},
  author = {Fruit, Ronan},
  langid = {english},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/DDEEDDIK/Fruit - Exploration-exploitation dilemma in Reinforcement .pdf}
}

@article{fu_model-based_nodate,
  title = {Model-Based {{Lifelong Reinforcement Learning}} with {{Bayesian Exploration}}},
  author = {Fu, Haotian and Yu, Shangqun and Littman, Michael and Konidaris, George},
  langid = {english},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/EYBSQIKW/Fu et al. - Model-based Lifelong Reinforcement Learning with B.pdf}
}

@article{grunwald_game_2004,
  title = {Game Theory, Maximum Entropy, Minimum Discrepancy and Robust {{Bayesian}} Decision Theory},
  author = {Grunwald, Peter D. and Dawid, A. Philip},
  year = {2004},
  month = aug,
  journal = {Ann. Statist.},
  volume = {32},
  number = {4},
  eprint = {math/0410076},
  issn = {0090-5364},
  doi = {10.1214/009053604000000553},
  urldate = {2024-02-23},
  abstract = {We describe and develop a close relationship between two problems that have customarily been regarded as distinct: that of maximizing entropy, and that of minimizing worst-case expected loss. Using a formulation grounded in the equilibrium theory of zero-sum games between Decision Maker and Nature, these two problems are shown to be dual to each other, the solution to each providing that to the other. Although Tops{\textbackslash}oe described this connection for the Shannon entropy over 20 years ago, it does not appear to be widely known even in that important special case. We here generalize this theory to apply to arbitrary decision problems and loss functions. We indicate how an appropriate generalized definition of entropy can be associated with such a problem, and we show that, subject to certain regularity conditions, the above-mentioned duality continues to apply in this extended context. This simultaneously provides a possible rationale for maximizing entropy and a tool for finding robust Bayes acts. We also describe the essential identity between the problem of maximizing entropy and that of minimizing a related discrepancy or divergence between distributions. This leads to an extension, to arbitrary discrepancies, of a well-known minimax theorem for the case of Kullback-Leibler divergence (the ``redundancy-capacity theorem'' of information theory). For the important case of families of distributions having certain mean values specified, we develop simple sufficient conditions and methods for identifying the desired solutions.},
  archiveprefix = {arxiv},
  keywords = {62C20 (Primary) 94A17 (Secondary),Mathematics - Statistics Theory},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/JF52PDWP/Grunwald and Dawid - 2004 - Game theory, maximum entropy, minimum discrepancy .pdf;/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/CE7WPM9X/0410076.html}
}

@article{hou_behavior_2022,
  title = {Behavior {{Reasoning}} for {{Opponent Agents}} in {{Multi-Agent Learning Systems}}},
  author = {Hou, Yaqing and Sun, Mingyang and Zhu, Wenxuan and Zeng, Yifeng and Piao, Haiyin and Chen, Xuefeng and Zhang, Qiang},
  year = {2022},
  month = oct,
  journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume = {6},
  number = {5},
  pages = {1125--1136},
  issn = {2471-285X},
  doi = {10.1109/TETCI.2022.3147011},
  urldate = {2024-02-19},
  abstract = {One important component of developing autonomous agents lies in the accurate prediction of their opponents' behaviors when the agents interact with others in an uncertain environment. Most recent study focuses on first constructing predictive types (or models) of the opponents, considering their various properties of interest, and subsequently using these models to predict their behaviors accordingly. However, as the possible type space can be rather large, it is time-consuming, and sometimes even infeasible, to predict the actual behaviors of opponents with all candidate types. Thus, in this paper a tractable opponent behavior reasoning approach is proposed that facilitates (a) extraction of a small yet representative summary of all candidates using sub-modular-type maximization, and accordingly, (b) identification of the most appropriate type for real-time behavior prediction based on multi-armed bandits. In addition, we propose a knowledge-transfer scheme through demonstration learning to synchronize subject agents' knowledge about their opponents' behaviors. This further reduces the burden of reasoning with all models of their opponents from the perspective of individual subject agents. We integrate the new behavior prediction and reasoning method into a state-of-the-art evolutionary multi-agent framework, namely a memetic multi-agent system (MeMAS), and demonstrate empirical performance in two problem domains.},
  keywords = {behavior prediction and reasoning,Cognition,Computational modeling,memetic computing,Memetics,multi-agent systems,Multi-agent systems,Opponent modeling,Predictive models,Real-time systems,Task analysis},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/5P6HJJ32/Hou et al. - 2022 - Behavior Reasoning for Opponent Agents in Multi-Ag.pdf}
}

@misc{kwonRLLatentMDPs2021,
  title = {{{RL}} for {{Latent MDPs}}: {{Regret Guarantees}} and a {{Lower Bound}}},
  shorttitle = {{{RL}} for {{Latent MDPs}}},
  author = {Kwon, Jeongyeol and Efroni, Yonathan and Caramanis, Constantine and Mannor, Shie},
  year = {2021},
  month = feb,
  number = {arXiv:2102.04939},
  eprint = {2102.04939},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.04939},
  urldate = {2023-05-25},
  abstract = {In this work, we consider the regret minimization problem for reinforcement learning in latent Markov Decision Processes (LMDP). In an LMDP, an MDP is randomly drawn from a set of \$M\$ possible MDPs at the beginning of the interaction, but the identity of the chosen MDP is not revealed to the agent. We first show that a general instance of LMDPs requires at least \${\textbackslash}Omega((SA)\^{}M)\$ episodes to even approximate the optimal policy. Then, we consider sufficient assumptions under which learning good policies requires polynomial number of episodes. We show that the key link is a notion of separation between the MDP system dynamics. With sufficient separation, we provide an efficient algorithm with local guarantee, \{{\textbackslash}it i.e.,\} providing a sublinear regret guarantee when we are given a good initialization. Finally, if we are given standard statistical sufficiency assumptions common in the Predictive State Representation (PSR) literature (e.g., Boots et al.) and a reachability assumption, we show that the need for initialization can be removed.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/2JVXSFYK/Kwon et al. - 2021 - RL for Latent MDPs Regret Guarantees and a Lower .pdf;/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/H3PMFVV6/2102.html}
}

@misc{laufferWhoNeedsKnow2023,
  title = {Who {{Needs}} to {{Know}}? {{Minimal Knowledge}} for {{Optimal Coordination}}},
  shorttitle = {Who {{Needs}} to {{Know}}?},
  author = {Lauffer, Niklas and Shah, Ameesh and Carroll, Micah and Dennis, Michael and Russell, Stuart},
  year = {2023},
  month = jul,
  number = {arXiv:2306.09309},
  eprint = {2306.09309},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.09309},
  urldate = {2023-11-26},
  abstract = {To optimally coordinate with others in cooperative games, it is often crucial to have information about one's collaborators: successful driving requires understanding which side of the road to drive on. However, not every feature of collaborators is strategically relevant: the fine-grained acceleration of drivers may be ignored while maintaining optimal coordination. We show that there is a well-defined dichotomy between strategically relevant and irrelevant information. Moreover, we show that, in dynamic games, this dichotomy has a compact representation that can be efficiently computed via a Bellman backup operator. We apply this algorithm to analyze the strategically relevant information for tasks in both a standard and a partially observable version of the Overcooked environment. Theoretical and empirical results show that our algorithms are significantly more efficient than baselines. Videos are available at https://minknowledge.github.io.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems,I.2.11,I.2.6},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/YVIKWCR7/Lauffer et al. - 2023 - Who Needs to Know Minimal Knowledge for Optimal C.pdf;/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/JGCT97CJ/2306.html}
}

@article{nyarko_bayesian_1998,
  title = {Bayesian {{Learning}} and {{Convergence}} to {{Nash Equilibria}} without {{Common Priors}}},
  author = {Nyarko, Yaw},
  year = {1998},
  journal = {Economic Theory},
  volume = {11},
  number = {3},
  eprint = {25055102},
  eprinttype = {jstor},
  pages = {643--655},
  publisher = {Springer},
  issn = {0938-2259},
  urldate = {2024-02-21},
  abstract = {Consider an infinitely repeated game where each player is characterized by a "type" which may be unknown to the other players in the game. Suppose further that each player's belief about others is independent of that player's type. Impose an absolute continuity condition on the ex ante beliefs of players (weaker than mutual absolute continuity). Then any limit point of beliefs of players about the future of the game conditional on the past lies in the set of Nash or Subjective equilibria. Our assumption does not require common priors so is weaker than Jordan (1991); however our conclusion is weaker, we obtain convergence to subjective and not necessarily Nash equilibria. Our model is a generalization of the Kalai and Lehrer (1993) model. Our assumption is weaker than theirs. However, our conclusion is also weaker, and shows that limit points of beliefs, and not actual play, are subjective equilibria.}
}

@inproceedings{o_donoghue_variational_2021,
  title = {Variational {{Bayesian Reinforcement Learning}} with {{Regret Bounds}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {O' Donoghue, Brendan},
  year = {2021},
  volume = {34},
  pages = {28208--28221},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-04-10},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/WK24BNEW/O' Donoghue - 2021 - Variational Bayesian Reinforcement Learning with R.pdf}
}

@book{oliehoek_concise_2016,
  title = {A {{Concise Introduction}} to {{Decentralized POMDPs}}},
  author = {Oliehoek, Frans A. and Amato, Christopher},
  year = {2016},
  series = {{{SpringerBriefs}} in {{Intelligent Systems}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-28929-8},
  urldate = {2024-03-04},
  isbn = {978-3-319-28927-4 978-3-319-28929-8},
  langid = {english},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/F4Q4NFXC/Oliehoek and Amato - 2016 - A Concise Introduction to Decentralized POMDPs.pdf}
}

@article{rahman_generating_2023,
  title = {Generating {{Teammates}} for {{Training Robust Ad Hoc Teamwork Agents}} via {{Best-Response Diversity}}},
  author = {Rahman, Arrasy and Fosong, Elliot and Carlucho, Ignacio and Albrecht, Stefano V.},
  year = {2023},
  month = may,
  journal = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  urldate = {2023-05-29},
  abstract = {Ad hoc teamwork (AHT) is the challenge of designing a robust learner agent that effectively collaborates with unknown teammates without prior coordination mechanisms. Early approaches address the AHT challenge by training the learner with a diverse set of handcrafted teammate policies, usually designed based on an expert's domain knowledge about the policies the learner may encounter. However, implementing teammate policies for training based on domain knowledge is not always feasible. In such cases, recent approaches attempted to improve the robustness of the learner by training it with teammate policies generated by optimising information-theoretic diversity metrics. The problem with optimising existing information-theoretic diversity metrics for teammate policy generation is the emergence of superficially different teammates. When used for AHT training, superficially different teammate behaviours may not improve a learner's robustness during collaboration with unknown teammates. In this paper, we present an automated teammate policy generation method optimising the Best-Response Diversity (BRDiv) metric, which measures diversity based on the compatibility of teammate policies in terms of returns. We evaluate our approach in environments with multiple valid coordination strategies, comparing against methods optimising information-theoretic diversity metrics and an ablation not optimising any diversity metric. Our experiments indicate that optimising BRDiv yields a diverse set of training teammate policies that improve the learner's performance relative to previous teammate generation approaches when collaborating with near-optimal previously unseen teammate policies.},
  langid = {english},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/HIF57VY8/Rahman et al. - 2023 - Generating Teammates for Training Robust Ad Hoc Te.pdf}
}

@misc{rahmanMinimumCoverageSets2023,
  title = {Minimum {{Coverage Sets}} for {{Training Robust Ad Hoc Teamwork Agents}}},
  author = {Rahman, Arrasy and Cui, Jiaxun and Stone, Peter},
  year = {2023},
  month = aug,
  number = {arXiv:2308.09595},
  eprint = {2308.09595},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.09595},
  urldate = {2023-11-09},
  abstract = {Robustly cooperating with unseen agents and human partners presents significant challenges due to the diverse cooperative conventions these partners may adopt. Existing Ad Hoc Teamwork (AHT) methods address this challenge by training an agent with a population of diverse teammate policies obtained through maximizing specific diversity metrics. However, these heuristic diversity metrics do not always maximize the agent's robustness in all cooperative problems. In this work, we first propose that maximizing an AHT agent's robustness requires it to emulate policies in the minimum coverage set (MCS), the set of best-response policies to any partner policies in the environment. We then introduce the L-BRDiv algorithm that generates a set of teammate policies that, when used for AHT training, encourage agents to emulate policies from the MCS. L-BRDiv works by solving a constrained optimization problem to jointly train teammate policies for AHT training and approximating AHT agent policies that are members of the MCS. We empirically demonstrate that L-BRDiv produces more robust AHT agents than state-of-the-art methods in a broader range of two-player cooperative problems without the need for extensive hyperparameter tuning for its objectives. Our study shows that L-BRDiv outperforms the baseline methods by prioritizing discovering distinct members of the MCS instead of repeatedly finding redundant policies.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/AXSHYA3F/Rahman et al. - 2023 - Minimum Coverage Sets for Training Robust Ad Hoc T.pdf;/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/H2PC5K6K/2308.html}
}

@article{ramamoorthy_latent-variable_nodate,
  title = {Latent-Variable {{MDP}} Models for Adapting the Interaction Environment of Diverse Users},
  author = {Ramamoorthy, Subramanian and Mahmud, M M Hassan and Rosman, Benjamin and Kohli, Pushmeet},
  abstract = {Interactive interfaces are a common feature of many systems ranging from field robotics to video games. In most applications, these interfaces must be used by a heterogeneous set of users, with substantial variety in effectiveness with the same interface when configured differently. We address the problem of personalizing such an interface, adapting parameters to present the user with an environment that is optimal with respect to their individual traits - enabling that particular user to achieve their personal optimum. We model the user as a parameterised Markov Decision Process (MDP), wherein the transition dynamics within a task depends on the latent personality traits (e.g., skill or dexterity) of the user. A key innovation is that we adapt at the level of action sets, picking a personalized optimal set of actions that the user should use. Our solution involves a latent variable formulation wherein we maintain beliefs over the latent type of users, which serves as a proxy for the hidden personality traits. This allows us to compute a Bayes optimal action set which when presented to the user allows them to achieve optimal performance. Our experiments, with real and simulated human participants, demonstrate that our personalized adaptive solution outperforms any alternate static solution, and also other adaptive algorithms such as EXP-3. Furthermore, we show that our algorithm is most useful under high diversity in user base, where the benefits of safe initialization and quick adaptation (properties our algorithm provably enjoys) are most pronounced.},
  langid = {english},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/IZTF9I27/Ramamoorthy et al. - Latent-variable MDP models for adapting the intera.pdf}
}

@article{ramirez_goal_nodate,
  title = {Goal {{Recognition}} over {{POMDPs}}: {{Inferring}} the {{Intention}} of a {{POMDP Agent}}},
  author = {Ram{\i}rez, Miquel and Geffner, Hector},
  abstract = {Plan recognition is the problem of inferring the goals and plans of an agent from partial observations of her behavior. Recently, it has been shown that the problem can be formulated and solved using planners, reducing plan recognition to plan generation. In this work, we extend this model-based approach to plan recognition to the POMDP setting, where actions are stochastic and states are partially observable. The task is to infer a probability distribution over the possible goals of an agent whose behavior results from a POMDP model. The POMDP model is shared between agent and observer except for the true goal of the agent that is hidden to the observer. The observations are action sequences O that may contain gaps as some or even most of the actions done by the agent may not be observed. We show that the posterior goal distribution P (G{\textbar}O) can be computed from the value function VG(b) over beliefs b generated by the POMDP planner for each possible goal G. Some extensions of the basic framework are discussed, and a number of experiments are reported.},
  langid = {english},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/CVNP6DT6/RamÄ±rez and Geffner - Goal Recognition over POMDPs Inferring the Intent.pdf}
}

@article{rigter_minimax_2021,
  title = {Minimax {{Regret Optimisation}} for {{Robust Planning}} in {{Uncertain Markov Decision Processes}}},
  author = {Rigter, Marc and Lacerda, Bruno and Hawes, Nick},
  year = {2021},
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {13},
  pages = {11930--11938},
  issn = {2374-3468},
  doi = {10.1609/aaai.v35i13.17417},
  urldate = {2024-03-08},
  abstract = {The parameters for a Markov Decision Process (MDP) often cannot be specified exactly. Uncertain MDPs (UMDPs) capture this model ambiguity by defining sets which the parameters belong to. Minimax regret has been proposed as an objective for planning in UMDPs to find robust policies which are not overly conservative. In this work, we focus on planning for Stochastic Shortest Path (SSP) UMDPs with uncertain cost and transition functions. We introduce a Bellman equation to compute the regret for a policy. We propose a dynamic programming algorithm that utilises the regret Bellman equation, and show that it optimises minimax regret exactly for UMDPs with independent uncertainties. For coupled uncertainties, we extend our approach to use options to enable a trade off between computation and solution quality. We evaluate our approach on both synthetic and real-world domains, showing that it significantly outperforms existing baselines.},
  copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Reinforcement Learning},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/WGMR6X3D/Rigter et al. - 2021 - Minimax Regret Optimisation for Robust Planning in.pdf}
}

@misc{sarkarDiverseConventionsHumanAI2023,
  title = {Diverse {{Conventions}} for {{Human-AI Collaboration}}},
  author = {Sarkar, Bidipta and Shih, Andy and Sadigh, Dorsa},
  year = {2023},
  month = oct,
  number = {arXiv:2310.15414},
  eprint = {2310.15414},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.15414},
  urldate = {2023-10-30},
  abstract = {Conventions are crucial for strong performance in cooperative multi-agent games, because they allow players to coordinate on a shared strategy without explicit communication. Unfortunately, standard multi-agent reinforcement learning techniques, such as self-play, converge to conventions that are arbitrary and non-diverse, leading to poor generalization when interacting with new partners. In this work, we present a technique for generating diverse conventions by (1) maximizing their rewards during self-play, while (2) minimizing their rewards when playing with previously discovered conventions (cross-play), stimulating conventions to be semantically different. To ensure that learned policies act in good faith despite the adversarial optimization of cross-play, we introduce {\textbackslash}emph\{mixed-play\}, where an initial state is randomly generated by sampling self-play and cross-play transitions and the player learns to maximize the self-play reward from this initial state. We analyze the benefits of our technique on various multi-agent collaborative games, including Overcooked, and find that our technique can adapt to the conventions of humans, surpassing human-level performance when paired with real users.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/3AARC6GD/Sarkar et al. - 2023 - Diverse Conventions for Human-AI Collaboration.pdf;/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/3J9NBN88/2310.html}
}

@article{shafipour_yourdshahi_-line_2022,
  title = {On-Line Estimators for Ad-Hoc Task Execution: Learning Types and Parameters of Teammates for Effective Teamwork},
  shorttitle = {On-Line Estimators for Ad-Hoc Task Execution},
  author = {Shafipour Yourdshahi, Elnaz and {do Carmo Alves}, Matheus Aparecido and Varma, Amokh and Soriano Marcolino, Leandro and Ueyama, J{\'o} and Angelov, Plamen},
  year = {2022},
  month = aug,
  journal = {Auton Agent Multi-Agent Syst},
  volume = {36},
  number = {2},
  pages = {45},
  issn = {1573-7454},
  doi = {10.1007/s10458-022-09571-9},
  urldate = {2024-02-19},
  abstract = {It is essential for agents to work together with others to accomplish common objectives, without pre-programmed coordination rules or previous knowledge of the current teammates, a challenge known as ad-hoc teamwork. In these systems, an agent estimates the algorithm of others in an on-line manner in order to decide its own actions for effective teamwork. A common approach is to assume a set of possible types and parameters for teammates, reducing the problem into estimating parameters and calculating distributions over types. Meanwhile, agents often must coordinate in a decentralised fashion to complete tasks that are displaced in an environment (e.g., in foraging, de-mining, rescue or fire control), where each member autonomously chooses which task to perform. By harnessing this knowledge, better estimation techniques can be developed. Hence, we present On-line Estimators for Ad-hoc Task Execution (OEATE), a novel algorithm for teammates' type and parameter estimation in decentralised task execution. We show theoretically that our algorithm can converge to perfect estimations, under some assumptions, as the number of tasks increases. Additionally, we run experiments for a diverse configuration set in the level-based foraging domain over full and partial observability, and in a ``capture the prey'' game. We obtain a lower error in parameter and type estimation than previous approaches and better performance in the number of completed tasks for some cases. In fact, we evaluate a variety of scenarios via the increasing number of agents, scenario sizes, number of items, and number of types, showing that we can overcome previous works in most cases considering the estimation process, besides robustness to an increasing number of types and even to an erroneous set of potential types.},
  langid = {english},
  keywords = {Ad-hoc teamwork,Decentralised task execution,Learning,Planning},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/6RRIEBCN/Shafipour Yourdshahi et al. - 2022 - On-line estimators for ad-hoc task execution lear.pdf}
}

@inproceedings{simchowitz_bayesian_2021,
  title = {Bayesian Decision-Making under Misspecified Priors with Applications to Meta-Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Simchowitz, Max and Tosh, Christopher and Krishnamurthy, Akshay and Hsu, Daniel J and Lykouris, Thodoris and Dudik, Miro and Schapire, Robert E},
  year = {2021},
  volume = {34},
  pages = {26382--26394},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-02-23},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/KYM9IF89/Simchowitz et al. - 2021 - Bayesian decision-making under misspecified priors.pdf}
}

@inproceedings{tian_towards_2020,
  title = {Towards {{Minimax Optimal Reinforcement Learning}} in {{Factored Markov Decision Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Tian, Yi and Qian, Jian and Sra, Suvrit},
  year = {2020},
  volume = {33},
  pages = {19896--19907},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-02-23},
  abstract = {We study minimax optimal reinforcement learning in episodic factored Markov decision processes (FMDPs), which are MDPs with conditionally independent transition components. Assuming the factorization is known, we propose two model-based algorithms. The first one achieves minimax optimal regret guarantees for a rich class of factored structures, while the second one enjoys better computational complexity with a slightly worse regret. A key new ingredient of our algorithms is the design of a bonus term to guide exploration. We complement our algorithms by presenting several structure dependent lower bounds on regret for FMDPs that reveal the difficulty hiding in the intricacy of the structures.},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/2AFI3TMH/Tian et al. - 2020 - Towards Minimax Optimal Reinforcement Learning in .pdf}
}

@article{toussaint_probabilistic_2006,
  title = {Probabilistic Inference for Solving ({{PO}}) {{MDPs}}},
  author = {Toussaint, Marc and Harmeling, Stefan and Storkey, Amos},
  year = {2006},
  month = dec,
  urldate = {2024-02-22},
  langid = {english},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/N6PV6623/Bu - School of Informatics, University of Edinburgh.pdf;/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/3QY3IVDB/probabilistic-inference-for-solving-po-mdps.html}
}

@misc{wang_online_2021,
  title = {Online {{Robust Reinforcement Learning}} with {{Model Uncertainty}}},
  author = {Wang, Yue and Zou, Shaofeng},
  year = {2021},
  month = oct,
  number = {arXiv:2109.14523},
  eprint = {2109.14523},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-10},
  abstract = {Robust reinforcement learning (RL) is to find a policy that optimizes the worst-case performance over an uncertainty set of MDPs. In this paper, we focus on model-free robust RL, where the uncertainty set is defined to be centering at a misspecified MDP that generates a single sample trajectory sequentially and is assumed to be unknown. We develop a sample-based approach to estimate the unknown uncertainty set and design a robust Q-learning algorithm (tabular case) and robust TDC algorithm (function approximation setting), which can be implemented in an online and incremental fashion. For the robust Q-learning algorithm, we prove that it converges to the optimal robust Q function, and for the robust TDC algorithm, we prove that it converges asymptotically to some stationary points. Unlike the results in [Roy et al., 2017], our algorithms do not need any additional conditions on the discount factor to guarantee the convergence. We further characterize the finite-time error bounds of the two algorithms and show that both the robust Q-learning and robust TDC algorithms converge as fast as their vanilla counterparts(within a constant factor). Our numerical experiments further demonstrate the robustness of our algorithms. Our approach can be readily extended to robustify many other algorithms, e.g., TD, SARSA, and other GTD algorithms.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/I8VJNRZI/Wang and Zou - 2021 - Online Robust Reinforcement Learning with Model Un.pdf;/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/KT75LL7Z/2109.html}
}

@article{wilson_multi-task_nodate,
  title = {Multi-{{Task Reinforcement Learning}}: {{A Hierarchical Bayesian Approach}}},
  author = {Wilson, Aaron},
  abstract = {We consider the problem of multi-task reinforcement learning, where the agent needs to solve a sequence of Markov Decision Processes (MDPs) chosen randomly from a fixed but unknown distribution. We model the distribution over MDPs using a hierarchical Bayesian infinite mixture model. For each novel MDP, we use the previously learned distribution as an informed prior for modelbased Bayesian reinforcement learning. The hierarchical Bayesian framework provides a strong prior that allows us to rapidly infer the characteristics of new environments based on previous environments, while the use of a nonparametric model allows us to quickly adapt to environments we have not encountered before. In addition, the use of infinite mixtures allows for the model to automatically learn the number of underlying MDP components. We evaluate our approach and show that it leads to significant speedups in convergence to an optimal policy after observing only a small number of tasks.},
  langid = {english},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/PPRS7BTF/Wilson - Multi-Task Reinforcement Learning A Hierarchical .pdf}
}

@misc{zand_fly_2022,
  title = {On-the-Fly {{Strategy Adaptation}} for Ad-Hoc {{Agent Coordination}}},
  author = {Zand, Jaleh and {Parker-Holder}, Jack and Roberts, Stephen J.},
  year = {2022},
  month = mar,
  number = {arXiv:2203.08015},
  eprint = {2203.08015},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.08015},
  urldate = {2023-04-23},
  abstract = {Training agents in cooperative settings offers the promise of AI agents able to interact effectively with humans (and other agents) in the real world. Multi-agent reinforcement learning (MARL) has the potential to achieve this goal, demonstrating success in a series of challenging problems. However, whilst these advances are significant, the vast majority of focus has been on the self-play paradigm. This often results in a coordination problem, caused by agents learning to make use of arbitrary conventions when playing with themselves. This means that even the strongest self-play agents may have very low cross-play with other agents, including other initializations of the same algorithm. In this paper we propose to solve this problem by adapting agent strategies on the fly, using a posterior belief over the other agents' strategy. Concretely, we consider the problem of selecting a strategy from a finite set of previously trained agents, to play with an unknown partner. We propose an extension of the classic statistical technique, Gibbs sampling, to update beliefs about other agents and obtain close to optimal ad-hoc performance. Despite its simplicity, our method is able to achieve strong cross-play with unseen partners in the challenging card game of Hanabi, achieving successful ad-hoc coordination without knowledge of the partner's strategy a priori.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/636DEP34/Zand et al. - 2022 - On-the-fly Strategy Adaptation for ad-hoc Agent Co.pdf;/Users/jackyansongli/Library/CloudStorage/Dropbox/zotero/storage/GDZVQ63T/2203.html}
}
