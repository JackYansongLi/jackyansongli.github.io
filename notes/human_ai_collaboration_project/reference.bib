@misc{agarwal_model-based_2022,
  title = {Model-Based {{RL}} with {{Optimistic Posterior Sampling}}: {{Structural Conditions}} and {{Sample Complexity}}},
  shorttitle = {Model-Based {{RL}} with {{Optimistic Posterior Sampling}}},
  author = {Agarwal, Alekh and Zhang, Tong},
  year = {2022},
  month = oct,
  number = {arXiv:2206.07659},
  eprint = {2206.07659},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.07659},
  urldate = {2023-06-27},
  abstract = {We propose a general framework to design posterior sampling methods for model-based RL. We show that the proposed algorithms can be analyzed by reducing regret to Hellinger distance in conditional probability estimation. We further show that optimistic posterior sampling can control this Hellinger distance, when we measure model error via data likelihood. This technique allows us to design and analyze unified posterior sampling algorithms with state-of-the-art sample complexity guarantees for many model-based RL settings. We illustrate our general result in many special cases, demonstrating the versatility of our framework.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/D3BV7DAZ/Agarwal and Zhang - 2022 - Model-based RL with Optimistic Posterior Sampling.pdf;/Users/yansongli/zotero/storage/IHI3R9X2/2206.html}
}

@inproceedings{agarwal_non-linear_2022,
  title = {Non-{{Linear Reinforcement Learning}} in {{Large Action Spaces}}: {{Structural Conditions}} and {{Sample-efficiency}} of {{Posterior Sampling}}},
  shorttitle = {Non-{{Linear Reinforcement Learning}} in {{Large Action Spaces}}},
  booktitle = {Proceedings of {{Thirty Fifth Conference}} on {{Learning Theory}}},
  author = {Agarwal, Alekh and Zhang, Tong},
  year = {2022},
  month = jun,
  pages = {2776--2814},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-09-07},
  abstract = {Provably sample-efficient Reinforcement Learning (RL) with rich observations and function approximation has witnessed tremendous recent progress, particularly when the underlying function approximators are linear. In this linear regime, computationally and statistically efficient methods exist where the potentially infinite state and action spaces can be captured through a known feature embedding, with the sample complexity scaling with the (intrinsic) dimension of these features. When the action space is finite, significantly more sophisticated results allow non-linear function approximation under appropriate structural constraints on the underlying RL problem, permitting for instance, the learning of good features instead of assuming access to them. In this work, we present the first result for non-linear function approximation which holds for general action spaces under a linear embeddability condition, which generalizes all linear and finite action settings. We design a novel optimistic posterior sampling strategy, TS\$\^3\$ for such problems. We further show worst case sample complexity guarantees that scale with a rank parameter of the RL problem, the linear embedding dimension introduced here and standard measures of function class complexity.},
  langid = {english},
  file = {/Users/yansongli/zotero/storage/SB9FDWUQ/Agarwal and Zhang - 2022 - Non-Linear Reinforcement Learning in Large Action .pdf}
}

@article{albrecht_autonomous_2018,
  title = {Autonomous Agents Modelling Other Agents: {{A}} Comprehensive Survey and Open Problems},
  shorttitle = {Autonomous Agents Modelling Other Agents},
  author = {Albrecht, Stefano V. and Stone, Peter},
  year = {2018},
  month = may,
  journal = {Artificial Intelligence},
  volume = {258},
  pages = {66--95},
  issn = {00043702},
  doi = {10.1016/j.artint.2018.01.002},
  urldate = {2023-04-27},
  langid = {english},
  file = {/Users/yansongli/zotero/storage/IFH3CE54/Albrecht and Stone - 2018 - Autonomous agents modelling other agents A compre.pdf}
}

@article{albrecht_belief_2016,
  title = {Belief and {{Truth}} in {{Hypothesised Behaviours}}},
  author = {Albrecht, Stefano V. and Crandall, Jacob W. and Ramamoorthy, Subramanian},
  year = {2016},
  month = jun,
  journal = {Artificial Intelligence},
  volume = {235},
  eprint = {1507.07688},
  primaryclass = {cs},
  pages = {63--94},
  issn = {00043702},
  doi = {10.1016/j.artint.2016.02.004},
  urldate = {2023-04-30},
  abstract = {There is a long history in game theory on the topic of Bayesian or "rational" learning, in which each player maintains beliefs over a set of alternative behaviours, or types, for the other players. This idea has gained increasing interest in the artificial intelligence (AI) community, where it is used as a method to control a single agent in a system composed of multiple agents with unknown behaviours. The idea is to hypothesise a set of types, each specifying a possible behaviour for the other agents, and to plan our own actions with respect to those types which we believe are most likely, given the observed actions of the agents. The game theory literature studies this idea primarily in the context of equilibrium attainment. In contrast, many AI applications have a focus on task completion and payoff maximisation. With this perspective in mind, we identify and address a spectrum of questions pertaining to belief and truth in hypothesised types. We formulate three basic ways to incorporate evidence into posterior beliefs and show when the resulting beliefs are correct, and when they may fail to be correct. Moreover, we demonstrate that prior beliefs can have a significant impact on our ability to maximise payoffs in the long-term, and that they can be computed automatically with consistent performance effects. Furthermore, we analyse the conditions under which we are able complete our task optimally, despite inaccuracies in the hypothesised types. Finally, we show how the correctness of hypothesised types can be ascertained during the interaction via an automated statistical analysis.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,I.2.11},
  file = {/Users/yansongli/zotero/storage/Y7VXWSQA/Albrecht et al. - 2016 - Belief and Truth in Hypothesised Behaviours.pdf;/Users/yansongli/zotero/storage/KEN4KWIM/1507.html}
}

@misc{albrecht_game-theoretic_2015,
  title = {A {{Game-Theoretic Model}} and {{Best-Response Learning Method}} for {{Ad Hoc Coordination}} in {{Multiagent Systems}}},
  author = {Albrecht, Stefano V. and Ramamoorthy, Subramanian},
  year = {2015},
  month = jun,
  number = {arXiv:1506.01170},
  eprint = {1506.01170},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1506.01170},
  urldate = {2023-04-30},
  abstract = {The ad hoc coordination problem is to design an autonomous agent which is able to achieve optimal flexibility and efficiency in a multiagent system with no mechanisms for prior coordination. We conceptualise this problem formally using a game-theoretic model, called the stochastic Bayesian game, in which the behaviour of a player is determined by its private information, or type. Based on this model, we derive a solution, called Harsanyi-Bellman Ad Hoc Coordination (HBA), which utilises the concept of Bayesian Nash equilibrium in a planning procedure to find optimal actions in the sense of Bellman optimal control. We evaluate HBA in a multiagent logistics domain called level-based foraging, showing that it achieves higher flexibility and efficiency than several alternative algorithms. We also report on a human-machine experiment at a public science exhibition in which the human participants played repeated Prisoner's Dilemma and Rock-Paper-Scissors against HBA and alternative algorithms, showing that HBA achieves equal efficiency and a significantly higher welfare and winning rate.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Multiagent Systems},
  file = {/Users/yansongli/zotero/storage/BKEGCN6F/Albrecht and Ramamoorthy - 2015 - A Game-Theoretic Model and Best-Response Learning .pdf;/Users/yansongli/zotero/storage/CYAM46GI/1506.html}
}

@inproceedings{auer_near-optimal_2008,
  title = {Near-Optimal {{Regret Bounds}} for {{Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
  year = {2008},
  volume = {21},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-09-07},
  abstract = {For undiscounted reinforcement learning in Markov decision processes (MDPs) we consider the total regret of a learning algorithm with respect to an optimal policy. In order to describe the transition structure of an MDP we propose a new parameter: An MDP has diameter D if for any pair of states s1,s2 there is a policy which moves from s1 to s2 in at most D steps (on average). We present a reinforcement learning algorithm with total regret O(DSAT) after T steps for any unknown MDP with S states, A actions per state, and diameter D. This bound holds with high probability. We also present a corresponding lower bound of Omega(DSAT) on the total regret of any learning algorithm. Both bounds demonstrate the utility of the diameter as structural parameter of the MDP.},
  file = {/Users/yansongli/zotero/storage/4U79D6KZ/Auer et al. - 2008 - Near-optimal Regret Bounds for Reinforcement Learn.pdf}
}

@misc{azar_minimax_2017,
  title = {Minimax {{Regret Bounds}} for {{Reinforcement Learning}}},
  author = {Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  year = {2017},
  month = jul,
  number = {arXiv:1703.05449},
  eprint = {1703.05449},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1703.05449},
  urldate = {2023-09-05},
  abstract = {We consider the problem of provably optimal exploration in reinforcement learning for finite horizon MDPs. We show that an optimistic modification to value iteration achieves a regret bound of \$\textbackslash tilde\{O\}( \textbackslash sqrt\{HSAT\} + H\^2S\^2A+H\textbackslash sqrt\{T\})\$ where \$H\$ is the time horizon, \$S\$ the number of states, \$A\$ the number of actions and \$T\$ the number of time-steps. This result improves over the best previous known bound \$\textbackslash tilde\{O\}(HS \textbackslash sqrt\{AT\})\$ achieved by the UCRL2 algorithm of Jaksch et al., 2010. The key significance of our new results is that when \$T\textbackslash geq H\^3S\^3A\$ and \$SA\textbackslash geq H\$, it leads to a regret of \$\textbackslash tilde\{O\}(\textbackslash sqrt\{HSAT\})\$ that matches the established lower bound of \$\textbackslash Omega(\textbackslash sqrt\{HSAT\})\$ up to a logarithmic factor. Our analysis contains two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in \$S\$), and we define Bernstein-based "exploration bonuses" that use the empirical variance of the estimated values at the next states (to improve scaling in \$H\$).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/JG2NLUX8/Azar et al. - 2017 - Minimax Regret Bounds for Reinforcement Learning.pdf;/Users/yansongli/zotero/storage/HBTRBFH6/1703.html}
}

@misc{azizzadenesheli_reinforcement_2016,
  title = {Reinforcement {{Learning}} of {{POMDPs}} Using {{Spectral Methods}}},
  author = {Azizzadenesheli, Kamyar and Lazaric, Alessandro and Anandkumar, Animashree},
  year = {2016},
  month = may,
  number = {arXiv:1602.07764},
  eprint = {1602.07764},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1602.07764},
  urldate = {2023-09-07},
  abstract = {We propose a new reinforcement learning algorithm for partially observable Markov decision processes (POMDP) based on spectral decomposition methods. While spectral methods have been previously employed for consistent learning of (passive) latent variable models such as hidden Markov models, POMDPs are more challenging since the learner interacts with the environment and possibly changes the future observations in the process. We devise a learning algorithm running through episodes, in each episode we employ spectral techniques to learn the POMDP parameters from a trajectory generated by a fixed policy. At the end of the episode, an optimization oracle returns the optimal memoryless planning policy which maximizes the expected reward based on the estimated POMDP model. We prove an order-optimal regret bound with respect to the optimal memoryless policy and efficient scaling with respect to the dimensionality of observation and action spaces.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/96X4QWXF/Azizzadenesheli et al. - 2016 - Reinforcement Learning of POMDPs using Spectral Me.pdf;/Users/yansongli/zotero/storage/GBGAZBPC/1602.html}
}

@article{barrett_making_2017,
  title = {Making Friends on the Fly: {{Cooperating}} with New Teammates},
  shorttitle = {Making Friends on the Fly},
  author = {Barrett, Samuel and Rosenfeld, Avi and Kraus, Sarit and Stone, Peter},
  year = {2017},
  month = jan,
  journal = {Artificial Intelligence},
  volume = {242},
  pages = {132--171},
  issn = {00043702},
  doi = {10.1016/j.artint.2016.10.005},
  urldate = {2023-04-30},
  langid = {english},
  file = {/Users/yansongli/zotero/storage/9R3LU8YF/Barrett et al. - 2017 - Making friends on the fly Cooperating with new te.pdf;/Users/yansongli/zotero/storage/ARWP2YG4/Barrett et al. - 2017 - Making friends on the fly Cooperating with new te.pdf}
}

@incollection{baumeister_survey_2022,
  title = {A {{Survey}} of {{Ad Hoc Teamwork Research}}},
  booktitle = {Multi-{{Agent Systems}}},
  author = {Mirsky, Reuth and Carlucho, Ignacio and Rahman, Arrasy and Fosong, Elliot and Macke, William and Sridharan, Mohan and Stone, Peter and Albrecht, Stefano V.},
  editor = {Baumeister, Dorothea and Rothe, J{\"o}rg},
  year = {2022},
  volume = {13442},
  pages = {275--293},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-20614-6_16},
  urldate = {2023-04-27},
  abstract = {Ad hoc teamwork is the research problem of designing agents that can collaborate with new teammates without prior coordination. This survey makes a two-fold contribution: First, it provides a structured description of the different facets of the ad hoc teamwork problem. Second, it discusses the progress that has been made in the field so far, and identifies the immediate and long-term open problems that need to be addressed in ad hoc teamwork.},
  isbn = {978-3-031-20613-9 978-3-031-20614-6},
  langid = {english},
  file = {/Users/yansongli/zotero/storage/9YM8QPDM/Mirsky et al. - 2022 - A Survey of Ad Hoc Teamwork Research.pdf}
}

@inproceedings{ben-davidAgnosticOnlineLearning2009,
  title = {Agnostic {{Online Learning}}},
  booktitle = {Annual {{Conference Computational Learning Theory}}},
  author = {{Ben-David}, Shai and P{\'a}l, D. and {Shalev-Shwartz}, S.},
  year = {2009},
  urldate = {2023-09-07},
  abstract = {We study learnability of hypotheses classes in agnostic online prediction models. The analogous question in the PAC learning model [Valiant, 1984] was addressed by Haussler [1992] and others, who showed that the VC dimension characterization of the sample complexity of learnability extends to the agnostic (or ''unrealizable'') setting. In his influential work, Littlestone [1988] described a combinatorial characterization of hypothesis classes that are learnable in the online model. We extend Littlestone's results in two aspects. First, while Littlestone only dealt with the realizable case, namely, assuming there exists a hypothesis in the class that perfectly explains the entire data, we derive results for the non-realizable (agnostic) case as well. In particular, we describe several models of non-realizable data and derive upper and lower bounds on the achievable regret. Second, we extend the theory to include margin-based hypothesis classes, in which the prediction of each hypothesis is accompanied by a confidence value. We demonstrate how the newly developed theory seamlessly yields novel online regret bounds for the important class of large margin linear separators.},
  file = {/Users/yansongli/zotero/storage/U2HG2ALX/Ben-David et al. - 2009 - Agnostic Online Learning.pdf}
}

@misc{brunskill_sample_2013,
  title = {Sample {{Complexity}} of {{Multi-task Reinforcement Learning}}},
  author = {Brunskill, Emma and Li, Lihong},
  year = {2013},
  month = sep,
  number = {arXiv:1309.6821},
  eprint = {1309.6821},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1309.6821},
  urldate = {2023-05-25},
  abstract = {Transferring knowledge across a sequence of reinforcement-learning tasks is challenging, and has a number of important applications. Though there is encouraging empirical evidence that transfer can improve performance in subsequent reinforcement-learning tasks, there has been very little theoretical analysis. In this paper, we introduce a new multi-task algorithm for a sequence of reinforcement-learning tasks when each task is sampled independently from (an unknown) distribution over a finite set of Markov decision processes whose parameters are initially unknown. For this setting, we prove under certain assumptions that the per-task sample complexity of exploration is reduced significantly due to transfer compared to standard single-task algorithms. Our multi-task algorithm also has the desired characteristic that it is guaranteed not to exhibit negative transfer: in the worst case its per-task sample complexity is comparable to the corresponding single-task algorithm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/WL2B6WN7/Brunskill and Li - 2013 - Sample Complexity of Multi-task Reinforcement Lear.pdf;/Users/yansongli/zotero/storage/KPQ7D685/1309.html}
}

@article{bubeck_regret_2012,
  title = {Regret {{Analysis}} of {{Stochastic}} and {{Nonstochastic Multi-armed Bandit Problems}}},
  author = {Bubeck, S{\'e}bastien},
  year = {2012},
  journal = {FNT in Machine Learning},
  volume = {5},
  number = {1},
  pages = {1--122},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000024},
  urldate = {2023-06-28},
  abstract = {Multi-armed bandit problems are the most basic examples of sequential decision problems with an exploration\textendash exploitation trade-off. This is the balance between staying with the option that gave highest payoffs in the past and exploring new options that might give higher payoffs in the future. Although the study of bandit problems dates back to the 1930s, exploration\textendash exploitation trade-offs arise in several modern applications, such as ad placement, website optimization, and packet routing. Mathematically, a multi-armed bandit is defined by the payoff process associated with each option. In this monograph, we focus on two extreme cases in which the analysis of regret is particularly simple and elegant: i.i.d. payoffs and adversarial payoffs. Besides the basic setting of finitely many actions, we also analyze some of the most important variants and extensions, such as the contextual bandit model.},
  langid = {english},
  file = {/Users/yansongli/zotero/storage/3BEHYJ5P/Bubeck - 2012 - Regret Analysis of Stochastic and Nonstochastic Mu.pdf}
}

@article{buchholz_computation_2019,
  title = {Computation of Weighted Sums of Rewards for Concurrent {{MDPs}}},
  author = {Buchholz, Peter and Scheftelowitsch, Dimitri},
  year = {2019},
  month = feb,
  journal = {Math Meth Oper Res},
  volume = {89},
  number = {1},
  pages = {1--42},
  issn = {1432-5217},
  doi = {10.1007/s00186-018-0653-1},
  urldate = {2023-09-07},
  abstract = {We consider sets of Markov decision processes (MDPs) with shared state and action spaces and assume that the individual MDPs in such a set represent different scenarios for a system's operation. In this setting, we solve the problem of finding a single policy that performs well under each of these scenarios by considering the weighted sum of value vectors for each of the scenarios. Several solution approaches as well as the general complexity of the problem are discussed and algorithms that are based on these solution approaches are presented. Finally, we compare the derived algorithms on a set of benchmark problems.},
  langid = {english},
  keywords = {Markov decision processes,Multi-objective optimization,Non-linear programming,Optimization},
  file = {/Users/yansongli/zotero/storage/3PNZXK5Z/Buchholz and Scheftelowitsch - 2019 - Computation of weighted sums of rewards for concur.pdf}
}

@article{carroll_utility_2019,
  title = {On the {{Utility}} of {{Learning}} about {{Humans}} for {{Human-AI Coordination}}},
  author = {Carroll, Micah and Shah, Rohin and Ho, Mark K. and Griffiths, Thomas L. and Seshia, Sanjit A. and Abbeel, Pieter and Dragan, Anca},
  year = {2019},
  month = oct,
  doi = {10.48550/arXiv.1910.05789},
  urldate = {2022-10-11},
  abstract = {While we would like agents that can coordinate with humans, current algorithms such as self-play and population-based training create agents that can coordinate with themselves. Agents that assume their partner to be optimal or similar to them can converge to coordination protocols that fail to understand and be understood by humans. To demonstrate this, we introduce a simple environment that requires challenging coordination, based on the popular game Overcooked, and learn a simple model that mimics human play. We evaluate the performance of agents trained via self-play and population-based training. These agents perform very well when paired with themselves, but when paired with our human model, they are significantly worse than agents designed to play with the human model. An experiment with a planning algorithm yields the same conclusion, though only when the human-aware planner is given the exact human model that it is playing with. A user study with real humans shows this pattern as well, though less strongly. Qualitatively, we find that the gains come from having the agent adapt to the human's gameplay. Given this result, we suggest several approaches for designing agents that learn about humans in order to better coordinate with them. Code is available at https://github.com/HumanCompatibleAI/overcooked\_ai.},
  langid = {english},
  file = {/Users/yansongli/zotero/storage/LF7V76C9/Carroll et al. - 2019 - On the Utility of Learning about Humans for Human-.pdf;/Users/yansongli/zotero/storage/IPUU7YF6/1910.html}
}

@article{chades_momdps_2012,
  title = {{{MOMDPs}}: {{A Solution}} for {{Modelling Adaptive Management Problems}}},
  shorttitle = {{{MOMDPs}}},
  author = {Chades, Iadine and Carwardine, Josie and Martin, Tara and Nicol, Samuel and Sabbadin, Regis and Buffet, Olivier},
  year = {2012},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {26},
  number = {1},
  pages = {267--273},
  issn = {2374-3468},
  doi = {10.1609/aaai.v26i1.8171},
  urldate = {2023-09-07},
  abstract = {In conservation biology and natural resource management, adaptive management is an iterative process of improving management by reducing uncertainty via monitoring. Adaptive management is the principal tool for conserving endangered species under global change, yet adaptive management problems suffer from a poor suite of solution methods. The common approach used to solve an adaptive management problem is to assume the system state is known and the system dynamics can be one of a set of pre-defined models. The solution method used is unsatisfactory, employing value iteration on a discretized belief MDP which restricts the study to very small problems. We show how to overcome this limitation by modelling an adaptive management problem as a restricted Mixed Observability MDP called hidden model MDP (hmMDP). We demonstrate how to simplify the value function, the backup operator and the belief update computation. We show that, although a simplified case of POMDPs, hm-MDPs are PSPACE-complete in the finite-horizon case. We illustrate the use of this model to manage a population of the threatened Gouldian finch, a bird species endemic to Northern Australia. Our simple modelling approach is an important step towards efficient algorithms for solving adaptive management problems.},
  copyright = {Copyright (c) 2021 Proceedings of the AAAI Conference on Artificial Intelligence},
  langid = {english},
  keywords = {hmMDP},
  file = {/Users/yansongli/zotero/storage/SKB9ZDMD/Chades et al. - 2012 - MOMDPs A Solution for Modelling Adaptive Manageme.pdf}
}

@misc{dong_q-learning_2019,
  title = {Q-Learning with {{UCB Exploration}} Is {{Sample Efficient}} for {{Infinite-Horizon MDP}}},
  author = {Dong, Kefan and Wang, Yuanhao and Chen, Xiaoyu and Wang, Liwei},
  year = {2019},
  month = sep,
  number = {arXiv:1901.09311},
  eprint = {1901.09311},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-06-19},
  abstract = {A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently, Jin et al. \textbackslash cite\{jin2018q\} proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \textbackslash emph\{without\} accessing a generative model. We show that the \textbackslash textit\{sample complexity of exploration\} of our algorithm is bounded by \$\textbackslash tilde\{O\}(\{\textbackslash frac\{SA\}\{\textbackslash epsilon\^2(1-\textbackslash gamma)\^7\}\})\$. This improves the previously best known result of \$\textbackslash tilde\{O\}(\{\textbackslash frac\{SA\}\{\textbackslash epsilon\^4(1-\textbackslash gamma)\^8\}\})\$ in this setting achieved by delayed Q-learning \textbackslash cite\{strehl2006pac\}, and matches the lower bound in terms of \$\textbackslash epsilon\$ as well as \$S\$ and \$A\$ except for logarithmic factors.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/EKJU7637/Dong et al. - 2019 - Q-learning with UCB Exploration is Sample Efficien.pdf;/Users/yansongli/zotero/storage/E4I79C5A/1901.html}
}

@article{duff_optimal_2002,
  title = {Optimal Learning: {{Computational}} Procedures for {{Bayes}} -Adaptive {{Markov}} Decision Processes},
  shorttitle = {Optimal Learning},
  author = {Duff, Michael O'Gordon},
  year = {2002},
  month = jan,
  journal = {Doctoral Dissertations Available from Proquest},
  pages = {1--247},
  file = {/Users/yansongli/zotero/storage/I5N6Z72B/AAI3039353.html}
}

@article{gmytrasiewicz_framework_2005,
  title = {A {{Framework}} for {{Sequential Planning}} in {{Multi-Agent Settings}}},
  author = {Gmytrasiewicz, P. J. and Doshi, P.},
  year = {2005},
  month = jul,
  journal = {Journal of Artificial Intelligence Research},
  volume = {24},
  pages = {49--79},
  issn = {1076-9757},
  doi = {10.1613/jair.1579},
  urldate = {2023-04-26},
  abstract = {This paper extends the framework of partially observable Markov decision processes (POMDPs) to multi-agent settings by incorporating the notion of agent models into the state space.  Agents maintain beliefs over physical states of the environment and over models of other agents, and they use Bayesian updates to maintain their beliefs over time. The solutions map belief states to actions. Models of other agents may include their belief states and are related to agent types considered in games of incomplete information.  We express the agents' autonomy by postulating that their models are not directly manipulable or observable by other agents.  We show that important properties of POMDPs, such as convergence of value iteration, the rate of convergence, and piece-wise linearity and convexity of the value functions carry over to our framework.  Our approach complements a more traditional approach to interactive settings which uses Nash equilibria as a solution paradigm.  We seek to avoid some of the drawbacks of equilibria which may be non-unique and do not capture off-equilibrium behaviors.  We do so at the cost of having to represent, process and continuously revise models of other agents. Since the agent's beliefs may be arbitrarily nested, the optimal solutions to decision making problems are only asymptotically computable.  However, approximate belief updates and approximately optimal plans are computable. We illustrate our framework using a simple application domain, and we show examples of belief updates and value functions.},
  copyright = {Copyright (c)},
  langid = {english},
  file = {/Users/yansongli/zotero/storage/L7RAKGRF/Gmytrasiewicz and Doshi - 2005 - A Framework for Sequential Planning in Multi-Agent.pdf}
}

@inproceedings{guez_efficient_2012,
  title = {Efficient {{Bayes-adaptive}} Reinforcement Learning Using Sample-Based Search},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 1},
  author = {Guez, Arthur and Silver, David and Dayan, Peter},
  year = {2012},
  month = dec,
  series = {{{NIPS}}'12},
  pages = {1025--1033},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}},
  urldate = {2023-09-07},
  abstract = {Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, finding the resulting Bayes-optimal policies is notoriously taxing, since the search space becomes enormous. In this paper we introduce a tractable, sample-based method for approximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems - because it avoids expensive applications of Bayes rule within the search tree by lazily sampling models from the current beliefs. We illustrate the advantages of our approach by showing it working in an infinite state space domain which is qualitatively out of reach of almost all previous work in Bayesian exploration.},
  file = {/Users/yansongli/zotero/storage/D6AXNNMC/Guez et al. - EfÔ¨Åcient Bayes-Adaptive Reinforcement Learning usi.pdf}
}

@inproceedings{guo_pac_2016,
  title = {A {{PAC RL Algorithm}} for {{Episodic POMDPs}}},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Guo, Z. and Doroudi, Shayan and Brunskill, E.},
  year = {2016},
  month = may,
  urldate = {2023-09-07},
  abstract = {Many interesting real world domains involve reinforcement learning (RL) in partially observable environments. Efficient learning in such domains is important, but existing sample complexity bounds for partially observable RL are at least exponential in the episode length. We give, to our knowledge, the first partially observable RL algorithm with a polynomial bound on the number of episodes on which the algorithm may not achieve near-optimal performance. Our algorithm is suitable for an important class of episodic POMDPs. Our approach builds on recent advances in method of moments for latent variable model estimation.},
  file = {/Users/yansongli/zotero/storage/FRIHNLEL/Guo et al. - 2016 - A PAC RL Algorithm for Episodic POMDPs.pdf}
}

@misc{hallak_contextual_2015,
  title = {Contextual {{Markov Decision Processes}}},
  author = {Hallak, Assaf and Di Castro, Dotan and Mannor, Shie},
  year = {2015},
  month = feb,
  number = {arXiv:1502.02259},
  eprint = {1502.02259},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-05-25},
  abstract = {We consider a planning problem where the dynamics and rewards of the environment depend on a hidden static parameter referred to as the context. The objective is to learn a strategy that maximizes the accumulated reward across all contexts. The new model, called Contextual Markov Decision Process (CMDP), can model a customer's behavior when interacting with a website (the learner). The customer's behavior depends on gender, age, location, device, etc. Based on that behavior, the website objective is to determine customer characteristics, and to optimize the interaction between them. Our work focuses on one basic scenario--finite horizon with a small known number of possible contexts. We suggest a family of algorithms with provable guarantees that learn the underlying models and the latent contexts, and optimize the CMDPs. Bounds are obtained for specific naive implementations, and extensions of the framework are discussed, laying the ground for future research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/PG4LPUPF/Hallak et al. - 2015 - Contextual Markov Decision Processes.pdf;/Users/yansongli/zotero/storage/LMN6DTBN/1502.html}
}

@inproceedings{han_learning_2018,
  title = {Learning {{Others}}' {{Intentional Models}} in {{Multi-Agent Settings Using Interactive POMDPs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Han, Yanlin and Gmytrasiewicz, Piotr},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-04-26},
  abstract = {Interactive partially observable Markov decision processes (I-POMDPs) provide a principled framework for planning and acting in a partially observable, stochastic and multi-agent environment. It extends POMDPs to multi-agent settings by including models of other agents in the state space and forming a hierarchical belief structure. In order to predict other agents' actions using I-POMDPs, we propose an approach that effectively uses Bayesian inference and sequential Monte Carlo sampling to learn others' intentional models which ascribe to them beliefs, preferences and rationality in action selection. Empirical results show that our algorithm accurately learns models of the other agent and has superior performance than methods that use subintentional models. Our approach serves as a generalized Bayesian learning algorithm that learns other agents' beliefs, strategy levels, and transition, observation and reward functions. It also effectively mitigates the belief space complexity due to the nested belief hierarchy.},
  file = {/Users/yansongli/zotero/storage/23KUA24Q/Han and Gmytrasiewicz - 2018 - Learning Others' Intentional Models in Multi-Agent.pdf}
}

@misc{he_opponent_2016,
  title = {Opponent {{Modeling}} in {{Deep Reinforcement Learning}}},
  author = {He, He and {Boyd-Graber}, Jordan and Kwok, Kevin and Daum{\'e} III, Hal},
  year = {2016},
  month = sep,
  number = {arXiv:1609.05559},
  eprint = {1609.05559},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-17},
  abstract = {Opponent modeling is necessary in multi-agent settings where secondary agents with competing goals also adapt their strategies, yet it remains challenging because strategies interact with each other and change. Most previous work focuses on developing probabilistic models or parameterized strategies for specific applications. Inspired by the recent success of deep reinforcement learning, we present neural-based models that jointly learn a policy and the behavior of opponents. Instead of explicitly predicting the opponent's action, we encode observation of the opponents into a deep Q-Network (DQN); however, we retain explicit modeling (if desired) using multitasking. By using a Mixture-of-Experts architecture, our model automatically discovers different strategy patterns of opponents without extra supervision. We evaluate our models on a simulated soccer game and a popular trivia game, showing superior performance over DQN and its variants.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/yansongli/zotero/storage/8BKXDBD8/He et al. - 2016 - Opponent Modeling in Deep Reinforcement Learning.pdf;/Users/yansongli/zotero/storage/JH65LRS9/1609.html}
}

@misc{jiang_contextual_2016,
  title = {Contextual {{Decision Processes}} with {{Low Bellman Rank}} Are {{PAC-Learnable}}},
  author = {Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E.},
  year = {2016},
  month = dec,
  number = {arXiv:1610.09512},
  eprint = {1610.09512},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1610.09512},
  urldate = {2023-05-25},
  abstract = {This paper studies systematic exploration for reinforcement learning with rich observations and function approximation. We introduce a new model called contextual decision processes, that unifies and generalizes most prior settings. Our first contribution is a complexity measure, the Bellman rank, that we show enables tractable learning of near-optimal behavior in these processes and is naturally small for many well-studied reinforcement learning settings. Our second contribution is a new reinforcement learning algorithm that engages in systematic exploration to learn contextual decision processes with low Bellman rank. Our algorithm provably learns near-optimal behavior with a number of samples that is polynomial in all relevant parameters but independent of the number of unique observations. The approach uses Bellman error minimization with optimistic exploration and provides new insights into efficient exploration for reinforcement learning with function approximation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/GNVM7G9J/Jiang et al. - 2016 - Contextual Decision Processes with Low Bellman Ran.pdf;/Users/yansongli/zotero/storage/AFTMSML9/1610.html}
}

@misc{jin_bellman_2021,
  title = {Bellman {{Eluder Dimension}}: {{New Rich Classes}} of {{RL Problems}}, and {{Sample-Efficient Algorithms}}},
  shorttitle = {Bellman {{Eluder Dimension}}},
  author = {Jin, Chi and Liu, Qinghua and Miryoosefi, Sobhan},
  year = {2021},
  month = jul,
  number = {arXiv:2102.00815},
  eprint = {2102.00815},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.00815},
  urldate = {2023-07-18},
  abstract = {Finding the minimal structural assumptions that empower sample-efficient learning is one of the most important research directions in Reinforcement Learning (RL). This paper advances our understanding of this fundamental question by introducing a new complexity measure -- Bellman Eluder (BE) dimension. We show that the family of RL problems of low BE dimension is remarkably rich, which subsumes a vast majority of existing tractable RL problems including but not limited to tabular MDPs, linear MDPs, reactive POMDPs, low Bellman rank problems as well as low Eluder dimension problems. This paper further designs a new optimization-based algorithm -- GOLF, and reanalyzes a hypothesis elimination-based algorithm -- OLIVE (proposed in Jiang et al., 2017). We prove that both algorithms learn the near-optimal policies of low BE dimension problems in a number of samples that is polynomial in all relevant parameters, but independent of the size of state-action space. Our regret and sample complexity results match or improve the best existing results for several well-known subclasses of low BE dimension problems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/GF7RVNUW/Jin et al. - 2021 - Bellman Eluder Dimension New Rich Classes of RL P.pdf;/Users/yansongli/zotero/storage/DURVF25E/2102.html}
}

@misc{jin_is_2018,
  title = {Is {{Q-learning Provably Efficient}}?},
  author = {Jin, Chi and {Allen-Zhu}, Zeyuan and Bubeck, Sebastien and Jordan, Michael I.},
  year = {2018},
  month = jul,
  number = {arXiv:1807.03765},
  eprint = {1807.03765},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.03765},
  urldate = {2023-06-28},
  abstract = {Model-free reinforcement learning (RL) algorithms, such as Q-learning, directly parameterize and update value functions or policies without explicitly modeling the environment. They are typically simpler, more flexible to use, and thus more prevalent in modern deep RL than model-based approaches. However, empirical work has suggested that model-free algorithms may require more samples to learn [Deisenroth and Rasmussen 2011, Schulman et al. 2015]. The theoretical question of "whether model-free algorithms can be made sample efficient" is one of the most fundamental questions in RL, and remains unsolved even in the basic scenario with finitely many states and actions. We prove that, in an episodic MDP setting, Q-learning with UCB exploration achieves regret \$\textbackslash tilde\{O\}(\textbackslash sqrt\{H\^3 SAT\})\$, where \$S\$ and \$A\$ are the numbers of states and actions, \$H\$ is the number of steps per episode, and \$T\$ is the total number of steps. This sample efficiency matches the optimal regret that can be achieved by any model-based approach, up to a single \$\textbackslash sqrt\{H\}\$ factor. To the best of our knowledge, this is the first analysis in the model-free setting that establishes \$\textbackslash sqrt\{T\}\$ regret without requiring access to a "simulator."},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/JLC4U25H/Jin et al. - 2018 - Is Q-learning Provably Efficient.pdf;/Users/yansongli/zotero/storage/QJHTXYU9/1807.html}
}

@inproceedings{jin_provably_2020,
  title = {Provably Efficient Reinforcement Learning with Linear Function Approximation},
  booktitle = {Proceedings of {{Thirty Third Conference}} on {{Learning Theory}}},
  author = {Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I.},
  year = {2020},
  month = jul,
  pages = {2137--2143},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-09-07},
  abstract = {Modern Reinforcement Learning (RL) is commonly applied to practical problems with an enormous number of states, where \textbackslash emph\{function approximation\} must be deployed to approximate either the value function or the policy. The introduction of function approximation raises a fundamental set of challenges involving computational and statistical efficiency, especially given the need to manage the exploration/exploitation tradeoff. As a result, a core RL question remains open: how can we design provably efficient RL algorithms that incorporate function approximation? This question persists even in a basic setting with linear dynamics and linear rewards, for which only linear function approximation is needed. This paper presents the first provable RL algorithm with both polynomial runtime and polynomial sample complexity in this linear setting, without requiring a ``simulator'' or additional assumptions. Concretely, we prove that an optimistic modification of Least-Squares Value Iteration (LSVI)\textemdash a classical algorithm frequently studied in the linear setting\textemdash achieves O\textasciitilde (d3H3T------{$\surd$})O\textasciitilde (d3H3T)\textbackslash tilde\{\textbackslash mathcal\{O\}\}(\textbackslash sqrt\{d\^3H\^3T\}) regret, where ddd is the ambient dimension of feature space, HHH is the length of each episode, and TTT is the total number of steps. Importantly, such regret is independent of the number of states and actions.},
  langid = {english},
  file = {/Users/yansongli/zotero/storage/EJ2JFWY8/Jin et al. - 2020 - Provably efficient reinforcement learning with lin.pdf}
}

@article{kearns_near-optimal_nodate,
  title = {Near-{{Optimal Reinforcement Learning}} in {{Polynomial Time}}},
  author = {Kearns, Michael and Singh, Satinder},
  abstract = {We present new algorithms for reinforcement learning, and prove that they have polynomial bounds on the resources required to achieve near-optimal return in general Markov decision processes. After observing that the number of actions required to approach the optimal return is lower bounded by the mixing time T of the optimal policy  in the undiscounted case  or by the horizon time T  in the discounted case , we then give algorithms requiring a number of actions and total computation time that are only polynomial in T and the number of states, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is their explicit handling of the ExplorationExploitation trade-o .},
  langid = {english},
  file = {/Users/yansongli/zotero/storage/CUL5TYNJ/Kearns and Singh - Near-Optimal Reinforcement Learning in Polynomial .pdf}
}

@misc{kwon_rl_2021,
  title = {{{RL}} for {{Latent MDPs}}: {{Regret Guarantees}} and a {{Lower Bound}}},
  shorttitle = {{{RL}} for {{Latent MDPs}}},
  author = {Kwon, Jeongyeol and Efroni, Yonathan and Caramanis, Constantine and Mannor, Shie},
  year = {2021},
  month = feb,
  number = {arXiv:2102.04939},
  eprint = {2102.04939},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.04939},
  urldate = {2023-05-25},
  abstract = {In this work, we consider the regret minimization problem for reinforcement learning in latent Markov Decision Processes (LMDP). In an LMDP, an MDP is randomly drawn from a set of \$M\$ possible MDPs at the beginning of the interaction, but the identity of the chosen MDP is not revealed to the agent. We first show that a general instance of LMDPs requires at least \$\textbackslash Omega((SA)\^M)\$ episodes to even approximate the optimal policy. Then, we consider sufficient assumptions under which learning good policies requires polynomial number of episodes. We show that the key link is a notion of separation between the MDP system dynamics. With sufficient separation, we provide an efficient algorithm with local guarantee, \{\textbackslash it i.e.,\} providing a sublinear regret guarantee when we are given a good initialization. Finally, if we are given standard statistical sufficiency assumptions common in the Predictive State Representation (PSR) literature (e.g., Boots et al.) and a reachability assumption, we show that the need for initialization can be removed.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/yansongli/zotero/storage/2JVXSFYK/Kwon et al. - 2021 - RL for Latent MDPs Regret Guarantees and a Lower .pdf;/Users/yansongli/zotero/storage/H3PMFVV6/2102.html}
}

@article{lai_asymptotically_1985,
  title = {Asymptotically Efficient Adaptive Allocation Rules},
  author = {Lai, T. L and Robbins, Herbert},
  year = {1985},
  month = mar,
  journal = {Advances in Applied Mathematics},
  volume = {6},
  number = {1},
  pages = {4--22},
  issn = {0196-8858},
  doi = {10.1016/0196-8858(85)90002-8},
  urldate = {2023-09-07},
  file = {/Users/yansongli/zotero/storage/NQBF75BN/Lai and Robbins - 1985 - Asymptotically efficient adaptive allocation rules.pdf;/Users/yansongli/zotero/storage/FLEA53S6/0196885885900028.html}
}

@misc{liu_one_2023,
  title = {One {{Objective}} to {{Rule Them All}}: {{A Maximization Objective Fusing Estimation}} and {{Planning}} for {{Exploration}}},
  shorttitle = {One {{Objective}} to {{Rule Them All}}},
  author = {Liu, Zhihan and Lu, Miao and Xiong, Wei and Zhong, Han and Hu, Hao and Zhang, Shenao and Zheng, Sirui and Yang, Zhuoran and Wang, Zhaoran},
  year = {2023},
  month = may,
  number = {arXiv:2305.18258},
  eprint = {2305.18258},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.18258},
  urldate = {2023-06-03},
  abstract = {In online reinforcement learning (online RL), balancing exploration and exploitation is crucial for finding an optimal policy in a sample-efficient way. To achieve this, existing sample-efficient online RL algorithms typically consist of three components: estimation, planning, and exploration. However, in order to cope with general function approximators, most of them involve impractical algorithmic components to incentivize exploration, such as optimization within data-dependent level-sets or complicated sampling procedures. To address this challenge, we propose an easy-to-implement RL framework called \textbackslash textit\{Maximize to Explore\} (\textbackslash texttt\{MEX\}), which only needs to optimize \textbackslash emph\{unconstrainedly\} a single objective that integrates the estimation and planning components while balancing exploration and exploitation automatically. Theoretically, we prove that \textbackslash texttt\{MEX\} achieves a sublinear regret with general function approximations for Markov decision processes (MDP) and is further extendable to two-player zero-sum Markov games (MG). Meanwhile, we adapt deep RL baselines to design practical versions of \textbackslash texttt\{MEX\}, in both model-free and model-based manners, which can outperform baselines by a stable margin in various MuJoCo environments with sparse rewards. Compared with existing sample-efficient online RL algorithms with general function approximations, \textbackslash texttt\{MEX\} achieves similar sample efficiency while enjoying a lower computational cost and is more compatible with modern deep RL methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/DPZN5XT7/Liu et al. - 2023 - One Objective to Rule Them All A Maximization Obj.pdf;/Users/yansongli/zotero/storage/U3IZXPTD/2305.html}
}

@inproceedings{liu_pac_2016,
  title = {{{PAC Continuous State Online Multitask Reinforcement Learning}} with {{Identification}}},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Autonomous Agents}} \& {{Multiagent Systems}}},
  author = {Liu, Yao and Guo, Zhaohan and Brunskill, Emma},
  year = {2016},
  month = may,
  series = {{{AAMAS}} '16},
  pages = {438--446},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  address = {{Richland, SC}},
  urldate = {2023-09-07},
  abstract = {One key feature of a general intelligent autonomous agent is to be able to learn from past experience to improve future performance. In this paper we consider how an agent can leverage prior experience from performing reinforcement learning in order to learn faster in future tasks. We introduce the first, to our knowledge, probably approximately correct (PAC) RL algorithm COMRLI for sequential multi-task learning across a series of continuous-state, discrete-action RL tasks. We assume tasks are sampled from a finite number of clusters of Markov decision processes, and provide a bound on the number of steps on which the algorithm makes a suboptimal decision that is substantially smaller on later tasks. We also provide preliminary evidence to suggest our approach may be useful in practice, by showing encouraging simulation performance in a standard domain where it compares favorably to a state-of-the-art algorithm.},
  isbn = {978-1-4503-4239-1},
  keywords = {multi-task learning,online learning,pac-mdp,reinforcement learning,sample complexity},
  file = {/Users/yansongli/zotero/storage/E6WH86XD/Liu et al. - 2016 - PAC Continuous State Online Multitask Reinforcemen.pdf}
}

@inproceedings{osband_model-based_2014,
  title = {Model-Based {{Reinforcement Learning}} and the {{Eluder Dimension}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Osband, Ian and Van Roy, Benjamin},
  year = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-09-07},
  file = {/Users/yansongli/zotero/storage/KVBQGKXH/Osband and Van Roy - 2014 - Model-based Reinforcement Learning and the Eluder .pdf}
}

@article{pineau_anytime_2006,
  title = {Anytime {{Point-Based Approximations}} for {{Large POMDPs}}},
  author = {Pineau, J. and Gordon, G. and Thrun, S.},
  year = {2006},
  month = nov,
  journal = {jair},
  volume = {27},
  pages = {335--380},
  issn = {1076-9757},
  doi = {10.1613/jair.2078},
  urldate = {2023-04-13},
  abstract = {The Partially Observable Markov Decision Process has long been recognized as a rich framework for real-world planning and control problems, especially in robotics. However exact solutions in this framework are typically computationally intractable for all but the smallest problems. A well-known technique for speeding up POMDP solving involves performing value backups at specific belief points, rather than over the entire belief simplex. The efficiency of this approach, however, depends greatly on the selection of points. This paper presents a set of novel techniques for selecting informative belief points which work well in practice. The point selection procedure is combined with point-based value backups to form an effective anytime POMDP algorithm called Point-Based Value Iteration (PBVI). The first aim of this paper is to introduce this algorithm and present a theoretical analysis justifying the choice of belief selection technique. The second aim of this paper is to provide a thorough empirical comparison between PBVI and other state-of-the-art POMDP methods, in particular the Perseus algorithm, in an effort to highlight their similarities and differences. Evaluation is performed using both standard POMDP domains and realistic robotic tasks.},
  langid = {english},
  file = {/Users/yansongli/zotero/storage/SXKYSLSW/Pineau et al. - 2006 - Anytime Point-Based Approximations for Large POMDP.pdf}
}

@article{ross_agnostic_2012,
  title = {Agnostic {{System Identification}} for {{Model-Based Reinforcement Learning}}},
  author = {Ross, Stephane and Bagnell, J. Andrew},
  year = {2012},
  month = jul,
  journal = {arXiv:1203.1007 [cs, stat]},
  eprint = {1203.1007},
  primaryclass = {cs, stat},
  urldate = {2020-12-17},
  abstract = {A fundamental problem in control is to learn a model of a system from observations that is useful for controller synthesis. To provide good performance guarantees, existing methods must assume that the real system is in the class of models considered during learning. We present an iterative method with strong guarantees even in the agnostic case where the system is not in the class. In particular, we show that any no-regret online learning algorithm can be used to obtain a near-optimal policy, provided some model achieves low training error and access to a good exploration distribution. Our approach applies to both discrete and continuous domains. We demonstrate its efficacy and scalability on a challenging helicopter domain from the literature.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/5I7ZYL4Q/Ross and Bagnell - 2012 - Agnostic System Identification for Model-Based Rei.pdf}
}

@misc{russo_learning_2014,
  title = {Learning to {{Optimize Via Posterior Sampling}}},
  author = {Russo, Daniel and Van Roy, Benjamin},
  year = {2014},
  month = feb,
  number = {arXiv:1301.2609},
  eprint = {1301.2609},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1301.2609},
  urldate = {2023-09-07},
  abstract = {This paper considers the use of a simple posterior sampling algorithm to balance between exploration and exploitation when learning to optimize actions such as in multi-armed bandit problems. The algorithm, also known as Thompson Sampling, offers significant advantages over the popular upper confidence bound (UCB) approach, and can be applied to problems with finite or infinite action spaces and complicated relationships among action rewards. We make two theoretical contributions. The first establishes a connection between posterior sampling and UCB algorithms. This result lets us convert regret bounds developed for UCB algorithms into Bayesian regret bounds for posterior sampling. Our second theoretical contribution is a Bayesian regret bound for posterior sampling that applies broadly and can be specialized to many model classes. This bound depends on a new notion we refer to as the eluder dimension, which measures the degree of dependence among action rewards. Compared to UCB algorithm Bayesian regret bounds for specific model classes, our general bound matches the best available for linear models and is stronger than the best available for generalized linear models. Further, our analysis provides insight into performance advantages of posterior sampling, which are highlighted through simulation results that demonstrate performance surpassing recently proposed UCB algorithms.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/yansongli/zotero/storage/ITVX38F2/Russo and Van Roy - 2014 - Learning to Optimize Via Posterior Sampling.pdf;/Users/yansongli/zotero/storage/FWQJ66LM/1301.html}
}

@article{smallwood_optimal_1973,
  title = {The {{Optimal Control}} of {{Partially Observable Markov Processes Over}} a {{Finite Horizon}}},
  author = {Smallwood, Richard D. and Sondik, Edward J.},
  year = {1973},
  journal = {Operations Research},
  volume = {21},
  number = {5},
  eprint = {168926},
  eprinttype = {jstor},
  pages = {1071--1088},
  publisher = {{INFORMS}},
  issn = {0030-364X},
  urldate = {2023-09-07},
  abstract = {This paper formulates the optimal control problem for a class of mathematical models in which the system to be controlled is characterized by a finite-state discrete-time Markov process. The states of this internal process are not directly observable by the controller; rather, he has available a set of observable outputs that are only probabilistically related to the internal state of the system. The formulation is illustrated by a simple machine-maintenance example, and other specific application areas are also discussed. The paper demonstrates that, if there are only a finite number of control intervals remaining, then the optimal payoff function is a piecewise-linear, convex function of the current state probabilities of the internal Markov process. In addition, an algorithm for utilizing this property to calculate the optimal control policy and payoff function for any finite horizon is outlined. These results are illustrated by a numerical example for the machine-maintenance problem.},
  file = {/Users/yansongli/zotero/storage/XT2TSHP5/Smallwood and Sondik - 1973 - The Optimal Control of Partially Observable Markov.pdf}
}

@article{steimle_multi-model_2021,
  title = {Multi-Model {{Markov}} Decision Processes},
  author = {Steimle, Lauren N. and Kaufman, David L. and Denton, Brian T.},
  year = {2021},
  month = may,
  journal = {IISE Transactions},
  pages = {1--16},
  issn = {2472-5854, 2472-5862},
  doi = {10.1080/24725854.2021.1895454},
  urldate = {2023-09-07},
  abstract = {Markov decision processes (MDPs) have found success in many application areas that involve sequential decision making under uncertainty, including the evaluation and design of treatment and screening protocols for medical decision making. However, the data used to parameterize the model can influence what policies are recommended, and multiple competing data sources are common in many application areas, including medicine. In this article, we introduce the Multimodel Markov decision process (MMDP) which generalizes a standard MDP by allowing for multiple models of the rewards and transition probabilities. Solution of the MMDP generates a single policy that maximizes the weighted performance over all models. This approach allows the decision maker to explicitly trade-off conflicting sources of data while generating a policy of the same level of complexity for models that only consider a single source of data. We study the structural properties of this problem and show that it is at least NP-hard. We develop exact methods and fast approximation methods supported by error bounds. Finally, we illustrate the effectiveness and the scalability of our approach using a case study in preventative blood pressure and cholesterol management that accounts for conflicting published cardiovascular risk models.},
  langid = {english},
  file = {/Users/yansongli/zotero/storage/XD5ICPSJ/Steimle et al. - 2021 - Multi-model Markov decision processes.pdf}
}

@article{stone_ad_2010,
  title = {Ad {{Hoc Autonomous Agent Teams}}: {{Collaboration}} without {{Pre-Coordination}}},
  shorttitle = {Ad {{Hoc Autonomous Agent Teams}}},
  author = {Stone, Peter and Kaminka, Gal and Kraus, Sarit and Rosenschein, Jeffrey},
  year = {2010},
  month = jul,
  journal = {AAAI},
  volume = {24},
  number = {1},
  pages = {1504--1509},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v24i1.7529},
  urldate = {2023-09-07},
  abstract = {As autonomous agents proliferate in the real world, both in software and robotic settings, they will increasingly need to band together for cooperative activities with previously unfamiliar teammates. In such ad hoc team settings, team strategies cannot be developed a priori. Rather, an agent must be prepared to cooperate with many types of teammates: it must collaborate without pre-coordination. This paper challenges the AI community to develop theory and to implement prototypes of ad hoc team agents. It defines the concept of ad hoc team agents, specifies an evaluation paradigm, and provides examples of possible theoretical and empirical approaches to challenge. The goal is to encourage progress towards this ambitious, newly realistic, and increasingly important research goal.},
  langid = {english},
  file = {/Users/yansongli/zotero/storage/NVVTM8C3/Stone et al. - 2010 - Ad Hoc Autonomous Agent Teams Collaboration witho.pdf}
}

@inproceedings{sutton_policy_1999,
  title = {Policy {{Gradient Methods}} for {{Reinforcement Learning}} with {{Function Approximation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  year = {1999},
  volume = {12},
  publisher = {{MIT Press}},
  urldate = {2023-09-05},
  abstract = {Function  approximation  is  essential  to  reinforcement  learning,  but  the standard approach of approximating a  value function and deter(cid:173) mining  a  policy  from  it  has so  far  proven theoretically  intractable.  In this paper we explore an alternative approach in which the policy  is explicitly represented by its own function approximator,  indepen(cid:173) dent of the value function,  and is  updated according to the gradient  of expected reward with respect to the policy parameters.  Williams's  REINFORCE method and actor-critic methods are examples of this  approach.  Our  main  new  result  is  to  show  that  the  gradient  can  be  written  in  a  form  suitable  for  estimation  from  experience  aided  by  an  approximate  action-value  or  advantage  function.  Using  this  result,  we  prove for  the first  time that a  version  of policy  iteration  with arbitrary differentiable function approximation is convergent to  a  locally optimal policy.},
  file = {/Users/yansongli/zotero/storage/CUB62942/Sutton et al. - 1999 - Policy Gradient Methods for Reinforcement Learning.pdf}
}

@article{taylor_transfer_2009,
  title = {Transfer {{Learning}} for {{Reinforcement Learning Domains}}: {{A Survey}}},
  shorttitle = {Transfer {{Learning}} for {{Reinforcement Learning Domains}}},
  author = {Taylor, Matthew E. and Stone, Peter},
  year = {2009},
  month = dec,
  journal = {J. Mach. Learn. Res.},
  volume = {10},
  pages = {1633--1685},
  issn = {1532-4435},
  abstract = {The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.},
  file = {/Users/yansongli/zotero/storage/GQG7EN7G/Taylor and Stone - 2009 - Transfer Learning for Reinforcement Learning Domai.pdf}
}

@article{thompson_likelihood_1933,
  title = {On the {{Likelihood}} That {{One Unknown Probability Exceeds Another}} in {{View}} of the {{Evidence}} of {{Two Samples}}},
  author = {Thompson, William R.},
  year = {1933},
  journal = {Biometrika},
  volume = {25},
  number = {3/4},
  eprint = {2332286},
  eprinttype = {jstor},
  pages = {285--294},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2332286},
  urldate = {2023-09-07},
  file = {/Users/yansongli/zotero/storage/5Z75G53H/Thompson - 1933 - On the Likelihood that One Unknown Probability Exc.pdf}
}

@misc{wang_policy_2023,
  title = {Policy {{Gradient}} in {{Robust MDPs}} with {{Global Convergence Guarantee}}},
  author = {Wang, Qiuhao and Ho, Chin Pang and Petrik, Marek},
  year = {2023},
  month = jun,
  number = {arXiv:2212.10439},
  eprint = {2212.10439},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.10439},
  urldate = {2023-09-05},
  abstract = {Robust Markov decision processes (RMDPs) provide a promising framework for computing reliable policies in the face of model errors. Many successful reinforcement learning algorithms build on variations of policy-gradient methods, but adapting these methods to RMDPs has been challenging. As a result, the applicability of RMDPs to large, practical domains remains limited. This paper proposes a new Double-Loop Robust Policy Gradient (DRPG), the first generic policy gradient method for RMDPs. In contrast with prior robust policy gradient algorithms, DRPG monotonically reduces approximation errors to guarantee convergence to a globally optimal policy in tabular RMDPs. We introduce a novel parametric transition kernel and solve the inner loop robust policy via a gradient-based method. Finally, our numerical results demonstrate the utility of our new algorithm and confirm its global convergence properties.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/yansongli/zotero/storage/JYJ2PM83/Wang et al. - 2023 - Policy Gradient in Robust MDPs with Global Converg.pdf;/Users/yansongli/zotero/storage/Z3VLE9LJ/2212.html}
}

@article{white_bayesian_1969,
  title = {Bayesian {{Decision Problems}} and {{Markov Chains}}},
  author = {White, D. J.},
  year = {1969},
  month = jan,
  journal = {Royal Statistical Society. Journal. Series A: General},
  volume = {132},
  number = {1},
  pages = {106--107},
  issn = {0035-9238},
  doi = {10.2307/2343761},
  urldate = {2023-09-07},
  abstract = {3. Bayesian Decision Problems and Markov Chains. By J. J. Martin. New York and London, Wiley, 1967. xii, 202 p. 934{${''}$}. 103s.},
  file = {/Users/yansongli/zotero/storage/N6Q3NVWU/7107340.html}
}

@misc{xie_learning_2020,
  title = {Learning {{Latent Representations}} to {{Influence Multi-Agent Interaction}}},
  author = {Xie, Annie and Losey, Dylan P. and Tolsma, Ryan and Finn, Chelsea and Sadigh, Dorsa},
  year = {2020},
  month = nov,
  number = {arXiv:2011.06619},
  eprint = {2011.06619},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.06619},
  urldate = {2023-04-30},
  abstract = {Seamlessly interacting with humans or robots is hard because these agents are non-stationary. They update their policy in response to the ego agent's behavior, and the ego agent must anticipate these changes to co-adapt. Inspired by humans, we recognize that robots do not need to explicitly model every low-level action another agent will make; instead, we can capture the latent strategy of other agents through high-level representations. We propose a reinforcement learning-based framework for learning latent representations of an agent's policy, where the ego agent identifies the relationship between its behavior and the other agent's future strategy. The ego agent then leverages these latent dynamics to influence the other agent, purposely guiding them towards policies suitable for co-adaptation. Across several simulated domains and a real-world air hockey game, our approach outperforms the alternatives and learns to influence the other agent.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/yansongli/zotero/storage/3B4G55AN/Xie et al. - 2020 - Learning Latent Representations to Influence Multi.pdf;/Users/yansongli/zotero/storage/87Y2XMQI/2011.html}
}

@inproceedings{yang_sample-optimal_2019,
  title = {Sample-{{Optimal Parametric Q-Learning Using Linearly Additive Features}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Yang, Lin and Wang, Mengdi},
  year = {2019},
  month = may,
  pages = {6995--7004},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-09-07},
  abstract = {Consider a Markov decision process (MDP) that admits a set of state-action features, which can linearly express the process's probabilistic transition model. We propose a parametric Q-learning algorithm that finds an approximate-optimal policy using a sample size proportional to the feature dimension \$K\$ and invariant with respect to the size of the state space. To further improve its sample efficiency, we exploit the monotonicity property and intrinsic noise structure of the Bellman operator, provided the existence of anchor state-actions that imply implicit non-negativity in the feature space. We augment the algorithm using techniques of variance reduction, monotonicity preservation, and confidence bounds. It is proved to find a policy which is \$\textbackslash epsilon\$-optimal from any initial state with high probability using \$\textbackslash widetilde\{O\}(K/\textbackslash epsilon\^2(1-\textbackslash gamma)\^3)\$ sample transitions for arbitrarily large-scale MDP with a discount factor \$\textbackslash gamma\textbackslash in(0,1)\$. A matching information-theoretical lower bound is proved, confirming the sample optimality of the proposed method with respect to all parameters (up to polylog factors).},
  langid = {english},
  file = {/Users/yansongli/zotero/storage/HL3FXDZR/Yang and Wang - 2019 - Sample-Optimal Parametric Q-Learning Using Linearl.pdf}
}

@inproceedings{zanette_learning_2020,
  title = {Learning {{Near Optimal Policies}} with {{Low Inherent Bellman Error}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Zanette, Andrea and Lazaric, Alessandro and Kochenderfer, Mykel and Brunskill, Emma},
  year = {2020},
  month = nov,
  pages = {10978--10989},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-09-07},
  abstract = {We study the exploration problem with approximate linear action-value functions in episodic reinforcement learning under the notion of low inherent Bellman error, a condition normally employed to show convergence of approximate value iteration. First we relate this condition to other common frameworks and show that it is strictly more general than the low rank (or linear) MDP assumption of prior work. Second we provide an algorithm with a high probability regret bound \$\textbackslash widetilde O(\textbackslash sum\_\{t=1\}\^H d\_t \textbackslash sqrt\{K\} + \textbackslash sum\_\{t=1\}\^H \textbackslash sqrt\{d\_t\} \textbackslash IBE K)\$ where \$H\$ is the horizon, \$K\$ is the number of episodes, \$\textbackslash IBE\$ is the value if the inherent Bellman error and \$d\_t\$ is the feature dimension at timestep \$t\$. In addition, we show that the result is unimprovable beyond constants and logs by showing a matching lower bound. This has two important consequences: 1) it shows that exploration is possible using only \textbackslash emph\{batch assumptions\} with an algorithm that achieves the optimal statistical rate for the setting we consider, which is more general than prior work on low-rank MDPs 2) the lack of closedness (measured by the inherent Bellman error) is only amplified by \$\textbackslash sqrt\{d\_t\}\$ despite working in the online setting. Finally, the algorithm reduces to the celebrated \textbackslash textsc\{LinUCB\} when \$H=1\$ but with a different choice of the exploration parameter that allows handling misspecified contextual linear bandits. While computational tractability questions remain open for the MDP setting, this enriches the class of MDPs with a linear representation for the action-value function where statistically efficient reinforcement learning is possible.},
  langid = {english},
  file = {/Users/yansongli/zotero/storage/RFK82DEM/Zanette et al. - 2020 - Learning Near Optimal Policies with Low Inherent B.pdf}
}

@misc{zhang_feel-good_2021,
  title = {Feel-{{Good Thompson Sampling}} for {{Contextual Bandits}} and {{Reinforcement Learning}}},
  author = {Zhang, Tong},
  year = {2021},
  month = oct,
  number = {arXiv:2110.00871},
  eprint = {2110.00871},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.00871},
  urldate = {2023-09-07},
  abstract = {Thompson Sampling has been widely used for contextual bandit problems due to the flexibility of its modeling power. However, a general theory for this class of methods in the frequentist setting is still lacking. In this paper, we present a theoretical analysis of Thompson Sampling, with a focus on frequentist regret bounds. In this setting, we show that the standard Thompson Sampling is not aggressive enough in exploring new actions, leading to suboptimality in some pessimistic situations. A simple modification called Feel-Good Thompson Sampling, which favors high reward models more aggressively than the standard Thompson Sampling, is proposed to remedy this problem. We show that the theoretical framework can be used to derive Bayesian regret bounds for standard Thompson Sampling, and frequentist regret bounds for Feel-Good Thompson Sampling. It is shown that in both cases, we can reduce the bandit regret problem to online least squares regression estimation. For the frequentist analysis, the online least squares regression bound can be directly obtained using online aggregation techniques which have been well studied. The resulting bandit regret bound matches the minimax lower bound in the finite action case. Moreover, the analysis can be generalized to handle a class of linearly embeddable contextual bandit problems (which generalizes the popular linear contextual bandit model). The obtained result again matches the minimax lower bound. Finally we illustrate that the analysis can be extended to handle some MDP problems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/TEJ8IZW9/Zhang - 2021 - Feel-Good Thompson Sampling for Contextual Bandits.pdf;/Users/yansongli/zotero/storage/HYEJE85P/2110.html}
}

@misc{zhong_gec_2023,
  title = {{{GEC}}: {{A Unified Framework}} for {{Interactive Decision Making}} in {{MDP}}, {{POMDP}}, and {{Beyond}}},
  shorttitle = {{{GEC}}},
  author = {Zhong, Han and Xiong, Wei and Zheng, Sirui and Wang, Liwei and Wang, Zhaoran and Yang, Zhuoran and Zhang, Tong},
  year = {2023},
  month = jun,
  number = {arXiv:2211.01962},
  eprint = {2211.01962},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  urldate = {2023-07-18},
  abstract = {We study sample efficient reinforcement learning (RL) under the general framework of interactive decision making, which includes Markov decision process (MDP), partially observable Markov decision process (POMDP), and predictive state representation (PSR) as special cases. Toward finding the minimum assumption that empowers sample efficient learning, we propose a novel complexity measure, generalized eluder coefficient (GEC), which characterizes the fundamental tradeoff between exploration and exploitation in online interactive decision making. In specific, GEC captures the hardness of exploration by comparing the error of predicting the performance of the updated policy with the in-sample training error evaluated on the historical data. We show that RL problems with low GEC form a remarkably rich class, which subsumes low Bellman eluder dimension problems, bilinear class, low witness rank problems, PO-bilinear class, and generalized regular PSR, where generalized regular PSR, a new tractable PSR class identified by us, includes nearly all known tractable POMDPs and PSRs. Furthermore, in terms of algorithm design, we propose a generic posterior sampling algorithm, which can be implemented in both model-free and model-based fashion, under both fully observable and partially observable settings. The proposed algorithm modifies the standard posterior sampling algorithm in two aspects: (i) we use an optimistic prior distribution that biases towards hypotheses with higher values and (ii) a loglikelihood function is set to be the empirical loss evaluated on the historical data, where the choice of loss function supports both model-free and model-based learning. We prove that the proposed algorithm is sample efficient by establishing a sublinear regret upper bound in terms of GEC. In summary, we provide a new and unified understanding of both fully observable and partially observable RL.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/YAA92JK7/Zhong et al. - 2023 - GEC A Unified Framework for Interactive Decision .pdf;/Users/yansongli/zotero/storage/VU9PY7T9/2211.html}
}
